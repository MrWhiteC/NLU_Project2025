{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb7dcbb-62a7-4fa1-9153-9b04687f8005",
   "metadata": {},
   "source": [
    "# AI Chatbot for MITRE ATT&CK Threat Classification and Organizational Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8a4341-2225-439c-a2de-586359e79cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random, math, time\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defceda1-dc10-47c0-a36f-fb029e4b82ae",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b31310-d317-4d69-b7f8-f44023c29893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a93035e-5b1f-436d-9a04-61d8fab8e25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text1', 'labels'],\n",
      "        num_rows: 14936\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text1', 'labels'],\n",
      "        num_rows: 2630\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text1', 'labels'],\n",
      "        num_rows: 3170\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (replace 'your_dataset_name' with the actual name)\n",
    "dataset = load_dataset('tumeteor/Security-TTP-Mapping')\n",
    "\n",
    "# # Optionally, select a specific split or a subset\n",
    "# dataset = dataset['train']  # or 'test', depending on the split\n",
    "\n",
    "# # Optionally select a specific range\n",
    "# dataset = dataset.select(range(10000))\n",
    "\n",
    "# Display the dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f8b2f-2a0e-4491-8a61-b712175bdb57",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde6f1e2-81e6-4267-97e6-3d5baa724a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLEAD also dabbled with a short-lived, fileless version of their malware when it obtained an exploit for a Flash vulnerability (CVE-2015-5119) that was leaked during the Hacking Team breach\n",
      "['T1203']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][1000]['text1'])\n",
    "print(dataset['train'][1000]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca7ce94-f015-459a-bf42-07282ce4e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In older versions, Valak downloads the second stage JS and uses only one obfuscation technique: Base64. The newer versions use XOR in addition to Base64\n",
      "['T1027']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][5]['text1'])\n",
    "print(dataset['train'][5]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b316260-4782-40a7-8448-d04349ce623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The command processing function starts by substituting the main module name and path in the hosting process PEB, with the one of the default internet browser. The path of the main browser of the workstation is obtained by reading the registry value', 'Along the way, HermeticWiper’s more mundane operations provide us with further IOCs to monitor for. These include the momentary creation of the abused driver as well as a system service. It also modifies several registry keys, including setting the SYSTEM\\\\CurrentControlSet\\\\Control\\\\CrashControl CrashDumpEnabled key to 0, effectively disabling crash dumps before the abused driver’s execution starts', 'These Microsoft Office templates are hosted on a command and control server and the downloaded link is embedded in the first stage malicious document', 'Additionally, the IP 211[.]72 [.]242[.]120 is one of the hosts for the domain microsoftmse[.]com, which has been used by several KIVARS variants', 'When communicating with its C2 server, Psylo will use HTTPS with a unique user-agent of (notice the lack of a space between \"5.0\" and \"(Windows']\n",
      "[\"['T1057']\", \"['T1569.002']\", \"['T1584.004']\", \"['T1056.001', 'T1113']\", \"['T1071.001']\"]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][:5]['text1'])\n",
    "print(dataset['train'][:5]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2a67699-3b9b-496a-80da-ffd9f5caf82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [(row['text1'], row['labels']) for row in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1475077-7845-419f-a9c9-92dec7188f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The command processing function starts by substituting the main module name and path in the hosting process PEB, with the one of the default internet browser. The path of the main browser of the workstation is obtained by reading the registry value',\n",
       " \"['T1057']\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef27af-cf92-4834-823b-744b06f64f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd3b8d08-aa4c-4acc-92d9-8a7317e600c9",
   "metadata": {},
   "source": [
    "# 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bc57671-543b-465b-b94c-c9a46c638a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The command processing function starts by substituting the main module name and path in the hosting process PEB, with the one of the default internet browser. The path of the main browser of the workstation is obtained by reading the registry value\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize the text data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text1'], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "# Apply tokenization to train and validation datasets\n",
    "train_data = dataset['train'].map(tokenize_function, batched=True)\n",
    "val_data = dataset['validation'].map(tokenize_function, batched=True)\n",
    "test_data = dataset['test'].map(tokenize_function, batched=True)\n",
    "\n",
    "# Print an example to verify\n",
    "print(train_data[0]['text1'])  # It should show tokenized input\n",
    "\n",
    "# # Decode the tokenized text back to human-readable text\n",
    "# decoded_text = tokenizer.decode(train_data[0]['input_ids'], skip_special_tokens=True)\n",
    "# print(f\"Decoded Text: {decoded_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a1d8e17-904c-4be0-aaa3-c59159224364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'command', 'processing', 'function', 'starts', 'by', 'sub', '##stituting', 'the', 'main', 'module', 'name', 'and', 'path', 'in', 'the', 'hosting', 'process', 'pe', '##b', ',', 'with', 'the', 'one', 'of', 'the', 'default', 'internet', 'browser', '.', 'the', 'path', 'of', 'the', 'main', 'browser', 'of', 'the', 'works', '##tation', 'is', 'obtained', 'by', 'reading', 'the', 'registry', 'value', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentence\n",
    "text = \"The command processing function starts by substituting the main module name and path in the hosting process PEB, with the one of the default internet browser. The path of the main browser of the workstation is obtained by reading the registry value.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c846cd-706d-48cc-8fec-abe6e9cc3277",
   "metadata": {},
   "source": [
    "# 3. Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13415bff-df37-4e6d-955e-940b0b9266ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f7e15ccf324ca8834d07de97af412f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3cf2465cd24634aa6ff0a58d426f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text1': 'The command processing function starts by substituting the main module name and path in the hosting process PEB, with the one of the default internet browser. The path of the main browser of the workstation is obtained by reading the registry value', 'labels': 1375, 'input_ids': [101, 1996, 3094, 6364, 3853, 4627, 2011, 4942, 21532, 1996, 2364, 11336, 2171, 1998, 4130, 1999, 1996, 9936, 2832, 21877, 2497, 1010, 2007, 1996, 2028, 1997, 1996, 12398, 4274, 16602, 1012, 1996, 4130, 1997, 1996, 2364, 16602, 1997, 1996, 2573, 12516, 2003, 4663, 2011, 3752, 1996, 15584, 3643, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Extract unique labels (MITRE techniques) from both train and validation datasets\n",
    "labels = list(set(dataset['train']['labels']).union(set(dataset['validation']['labels'])))  # Extract unique labels\n",
    "label_map = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Function to encode labels into integers\n",
    "def encode_labels(examples):\n",
    "    # Safely map the labels, providing a default value if a label is not found in the label_map\n",
    "    examples['labels'] = [label_map.get(label, -1) for label in examples['labels']]\n",
    "    return examples\n",
    "\n",
    "# Apply label encoding\n",
    "train_data = train_data.map(encode_labels, batched=True)\n",
    "val_data = val_data.map(encode_labels, batched=True)\n",
    "\n",
    "# Print an example to verify\n",
    "print(train_data[0])  # It should show tokenized input along with the encoded label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5016848-0915-454b-91fd-1067ed34070b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d4e712e-de85-4e26-b54d-f85afd69907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    # If using softmax, we need to use argmax to get the final class prediction\n",
    "    preds = preds.argmax(axis=-1)\n",
    "    \n",
    "    # Calculate precision, recall, F1-score, and accuracy\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5510f-eabf-4501-b4b5-a6aa4b46216e",
   "metadata": {},
   "source": [
    "# 4.Training model with bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c52cc62f-f859-4108-b21d-17867264078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 07:16:09.938239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744182969.958876 2084154 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744182969.967835 2084154 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 07:16:09.999871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mst124945\u001b[0m (\u001b[33mst124945-asian-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter-st124945/NLP Project/wandb/run-20250409_071617-9xvlb5ea</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/st124945-asian-institute-of-technology/huggingface/runs/9xvlb5ea' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/st124945-asian-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/st124945-asian-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/st124945-asian-institute-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/st124945-asian-institute-of-technology/huggingface/runs/9xvlb5ea' target=\"_blank\">https://wandb.ai/st124945-asian-institute-of-technology/huggingface/runs/9xvlb5ea</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1401' max='1401' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1401/1401 11:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.249989</td>\n",
       "      <td>0.319392</td>\n",
       "      <td>0.162067</td>\n",
       "      <td>0.319392</td>\n",
       "      <td>0.201676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.094200</td>\n",
       "      <td>3.614886</td>\n",
       "      <td>0.414829</td>\n",
       "      <td>0.271528</td>\n",
       "      <td>0.414829</td>\n",
       "      <td>0.309434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.575500</td>\n",
       "      <td>3.438241</td>\n",
       "      <td>0.437643</td>\n",
       "      <td>0.292122</td>\n",
       "      <td>0.437643</td>\n",
       "      <td>0.333476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final_model/tokenizer_config.json',\n",
       " './final_model/special_tokens_map.json',\n",
       " './final_model/vocab.txt',\n",
       " './final_model/added_tokens.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(labels))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=8,   \n",
    "    per_device_eval_batch_size=16,   \n",
    "    evaluation_strategy=\"epoch\",     \n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The pre-trained model\n",
    "    args=training_args,                  # The training arguments\n",
    "    train_dataset=train_data,            # The training dataset\n",
    "    eval_dataset=val_data,               # The validation dataset\n",
    "    compute_metrics=compute_metrics      # Add the compute_metrics function\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Optionally save the final model manually (this step is usually not required as the trainer saves it automatically)\n",
    "trainer.save_model(\"./final_model\")  # You can specify any directory you prefer\n",
    "\n",
    "# Also save the tokenizer (if necessary)\n",
    "tokenizer.save_pretrained(\"./final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf00dad",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54df2d-0c38-42d8-9dd6-57a262613626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "eval_results = trainer.evaluate(eval_dataset=val_data)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049fea9-2bbd-406a-ae7e-c67bfaab3f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_data is defined and contains the test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_data)\n",
    "\n",
    "# Print the test results\n",
    "print(\"Test results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82693bfd-1155-4ba4-890f-8fc004cb204e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ef1e5a0-7864-4d6b-a320-10b78a5896fb",
   "metadata": {},
   "source": [
    "# 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bfec0e9-ac0a-4f05-af52-4c7b52223fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text for classification:  An Iranian state-sponsored actor has been observed scanning and attempting to abuse the Log4Shell flaw in publicly-exposed Java applications to deploy a hitherto undocumented PowerShell-based modular backdoor dubbed \"CharmPower\" for follow-on post-exploitation. \"The actor's attack setup was obviously rushed, as they used the basic open-source tool for the exploitation and based their operations on previous infrastructure, which made the attack easier to detect and attribute,\" researchers from Check Point said in a report published this week. The Israeli cybersecurity company linked the attack to a group known as APT35, which is also tracked using the codenames Charming Kitten, Phosphorus, and TA453, citing overlaps with toolsets previously identified as infrastructure used by the threat actor. Cybersecurity Log4Shell aka CVE-2021-44228 (CVSS score: 10.0) concerns a critical security vulnerability in the popular Log4j logging library that, if successfully exploited, could lead to remote execution of arbitrary code on compromised systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: ['T1518.001']\n",
      "Prediction Probabilities: tensor([[0.0001, 0.0002, 0.0002,  ..., 0.0001, 0.0001, 0.0006]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./final_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get prediction for manual input text\n",
    "def predict(input_text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Get the model's output\n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the logits (model output before applying softmax)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Apply softmax to get probabilities (for multi-class classification)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the predicted label (index of max probability)\n",
    "    predicted_label = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    return predicted_label, probabilities\n",
    "\n",
    "# Test with manual input\n",
    "input_text = input(\"Enter text for classification: \")\n",
    "\n",
    "predicted_label, probabilities = predict(input_text)\n",
    "\n",
    "# Assuming `labels` contains the label names and was created previously (e.g., from `dataset['train']['label']`)\n",
    "# You can map the predicted label index back to the label name\n",
    "predicted_label_name = labels[predicted_label]\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label_name}\")\n",
    "print(f\"Prediction Probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6541db21-437b-4bfc-91e0-2b5bf1452952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/jupyter-\n",
      "[nltk_data]     st124945/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "An Iranian state-sponsored actor has been observed scanning and attempting to abuse the Log4Shell flaw in publicly-exposed Java applications to deploy a hitherto undocumented PowerShell-based modular backdoor dubbed \"CharmPower\" for follow-on post-exploitation.\n",
      "Predicted Label: ['T1580']\n",
      "Prediction Probabilities: tensor([[0.0003, 0.0004, 0.0005,  ..., 0.0003, 0.0004, 0.0011]])\n",
      "\n",
      "Sentence: \"The actor's attack setup was obviously rushed, as they used the basic open-source tool for the exploitation and based their operations on previous infrastructure, which made the attack easier to detect and attribute,\" researchers from Check Point said in a report published this week.\n",
      "Predicted Label: ['T1064', 'T1547.001']\n",
      "Prediction Probabilities: tensor([[5.4603e-05, 8.6680e-05, 8.7713e-05,  ..., 7.2819e-05, 6.7793e-05,\n",
      "         4.5937e-04]])\n",
      "\n",
      "Sentence: The Israeli cybersecurity company linked the attack to a group known as APT35, which is also tracked using the codenames Charming Kitten, Phosphorus, and TA453, citing overlaps with toolsets previously identified as infrastructure used by the threat actor.\n",
      "Predicted Label: ['T1074.001']\n",
      "Prediction Probabilities: tensor([[5.4549e-05, 7.8427e-05, 8.7657e-05,  ..., 8.4172e-05, 5.8116e-05,\n",
      "         3.7422e-04]])\n",
      "\n",
      "Sentence: Cybersecurity Log4Shell aka CVE-2021-44228 (CVSS score: 10.0) concerns a critical security vulnerability in the popular Log4j logging library that, if successfully exploited, could lead to remote execution of arbitrary code on compromised systems.\n",
      "Predicted Label: ['T1518.001']\n",
      "Prediction Probabilities: tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0001, 0.0001, 0.0006]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./final_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Assuming you have a list of labels\n",
    "# labels = ['Label_0', 'Label_1', 'Label_2']  # Update with your actual labels\n",
    "\n",
    "# Function to get prediction for manual input text\n",
    "def predict(input_text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Get the model's output\n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the logits (model output before applying softmax)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Apply softmax to get probabilities (for multi-class classification)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the predicted label (index of max probability)\n",
    "    predicted_label = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    return predicted_label, probabilities\n",
    "\n",
    "# Function to break text into sentences using nltk\n",
    "def sentence_tokenizer(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Test with article input\n",
    "input_text = \"\"\"\n",
    "An Iranian state-sponsored actor has been observed scanning and attempting to abuse the Log4Shell flaw in publicly-exposed Java applications to deploy a hitherto undocumented PowerShell-based modular backdoor dubbed \"CharmPower\" for follow-on post-exploitation. \"The actor's attack setup was obviously rushed, as they used the basic open-source tool for the exploitation and based their operations on previous infrastructure, which made the attack easier to detect and attribute,\" researchers from Check Point said in a report published this week. The Israeli cybersecurity company linked the attack to a group known as APT35, which is also tracked using the codenames Charming Kitten, Phosphorus, and TA453, citing overlaps with toolsets previously identified as infrastructure used by the threat actor. Cybersecurity Log4Shell aka CVE-2021-44228 (CVSS score: 10.0) concerns a critical security vulnerability in the popular Log4j logging library that, if successfully exploited, could lead to remote execution of arbitrary code on compromised systems.\n",
    "\"\"\"\n",
    "\n",
    "# Break the article into sentences\n",
    "sentences = sentence_tokenizer(input_text)\n",
    "\n",
    "# Process each sentence through the model\n",
    "for sentence in sentences:\n",
    "    predicted_label, probabilities = predict(sentence)\n",
    "    \n",
    "    # Map the predicted label index to the label name\n",
    "    predicted_label_name = labels[predicted_label]\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Predicted Label: {predicted_label_name}\")\n",
    "    print(f\"Prediction Probabilities: {probabilities}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16fe92b1-8e87-4bdb-b731-d81aae939b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: An Iranian state-sponsored actor has been observed scanning and attempting to abuse the Log4Shell flaw in publicly-exposed Java applications to deploy a hitherto undocumented PowerShell-based modular backdoor dubbed \"CharmPower\" for follow-on post-exploitation.\n",
      "Predicted Label: ['T1580']\n",
      "Prediction Probabilities: tensor([[0.0003, 0.0004, 0.0005,  ..., 0.0003, 0.0004, 0.0011]])\n",
      "\n",
      "Sentence: \"The actor's attack setup was obviously rushed, as they used the basic open-source tool for the exploitation and based their operations on previous infrastructure, which made the attack easier to detect and attribute,\" researchers from Check Point said in a report published this week.\n",
      "Predicted Label: ['T1064', 'T1547.001']\n",
      "Prediction Probabilities: tensor([[5.4603e-05, 8.6680e-05, 8.7713e-05,  ..., 7.2819e-05, 6.7793e-05,\n",
      "         4.5937e-04]])\n",
      "\n",
      "Sentence: The Israeli cybersecurity company linked the attack to a group known as APT35, which is also tracked using the codenames Charming Kitten, Phosphorus, and TA453, citing overlaps with toolsets previously identified as infrastructure used by the threat actor.\n",
      "Predicted Label: ['T1074.001']\n",
      "Prediction Probabilities: tensor([[5.4549e-05, 7.8427e-05, 8.7657e-05,  ..., 8.4172e-05, 5.8116e-05,\n",
      "         3.7422e-04]])\n",
      "\n",
      "Sentence: Cybersecurity Log4Shell aka CVE-2021-44228 (CVSS score: 10.0) concerns a critical security vulnerability in the popular Log4j logging library that, if successfully exploited, could lead to remote execution of arbitrary code on compromised systems.\n",
      "Predicted Label: ['T1518.001']\n",
      "Prediction Probabilities: tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0001, 0.0001, 0.0006]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./final_model')  # Replace with your model path\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Replace with your tokenizer if needed\n",
    "\n",
    "# # Assuming you have a list of labels (e.g., ['Label_0', 'Label_1', 'Label_2'])\n",
    "# labels = model.config.id2label # Update with your actual labels\n",
    "# print(labels)\n",
    "# Function to get prediction for a given sentence\n",
    "def predict(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    predicted_label = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    return predicted_label, probabilities\n",
    "\n",
    "# Function to break text into sentences using spaCy\n",
    "def sentence_tokenizer_spacy(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]  # Strip leading/trailing spaces\n",
    "    return sentences\n",
    "\n",
    "# Example news article\n",
    "input_text = \"\"\"\n",
    "An Iranian state-sponsored actor has been observed scanning and attempting to abuse the Log4Shell flaw in publicly-exposed Java applications to deploy a hitherto undocumented PowerShell-based modular backdoor dubbed \"CharmPower\" for follow-on post-exploitation. \"The actor's attack setup was obviously rushed, as they used the basic open-source tool for the exploitation and based their operations on previous infrastructure, which made the attack easier to detect and attribute,\" researchers from Check Point said in a report published this week. The Israeli cybersecurity company linked the attack to a group known as APT35, which is also tracked using the codenames Charming Kitten, Phosphorus, and TA453, citing overlaps with toolsets previously identified as infrastructure used by the threat actor. Cybersecurity Log4Shell aka CVE-2021-44228 (CVSS score: 10.0) concerns a critical security vulnerability in the popular Log4j logging library that, if successfully exploited, could lead to remote execution of arbitrary code on compromised systems.\n",
    "\"\"\"\n",
    "\n",
    "# Break the article into sentences using spaCy\n",
    "sentences = sentence_tokenizer_spacy(input_text)\n",
    "\n",
    "# Process each sentence through the model and get predictions\n",
    "for sentence in sentences:\n",
    "    predicted_label, probabilities = predict(sentence)\n",
    "    \n",
    "    # Map the predicted label index to the label name\n",
    "    predicted_label_name = labels[predicted_label]\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Predicted Label: {predicted_label_name}\")\n",
    "    print(f\"Prediction Probabilities: {probabilities}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42606028-554a-464a-aedc-504255b78f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c5e04-d167-4b9b-aa1b-86fd596548ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a24f3d-1371-47d2-b75f-f099e896ac9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4df59-52f3-48b0-88b3-bbd538e3f244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
