{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb7dcbb-62a7-4fa1-9153-9b04687f8005",
   "metadata": {},
   "source": [
    "# AI Chatbot for MITRE ATT&CK Threat Classification and Organizational Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a8a4341-2225-439c-a2de-586359e79cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random, math, time\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defceda1-dc10-47c0-a36f-fb029e4b82ae",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8b31310-d317-4d69-b7f8-f44023c29893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cu121'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a93035e-5b1f-436d-9a04-61d8fab8e25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text1', 'labels'],\n",
      "        num_rows: 14936\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text1', 'labels'],\n",
      "        num_rows: 2630\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text1', 'labels'],\n",
      "        num_rows: 3170\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (replace 'your_dataset_name' with the actual name)\n",
    "dataset = load_dataset('tumeteor/Security-TTP-Mapping')\n",
    "\n",
    "# # Optionally, select a specific split or a subset\n",
    "# dataset = dataset['train']  # or 'test', depending on the split\n",
    "\n",
    "# # Optionally select a specific range\n",
    "# dataset = dataset.select(range(10000))\n",
    "\n",
    "# Display the dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bca7dc4-0861-4347-8818-1e5a2454d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# # Load your dataset\n",
    "# dataset = load_dataset('sarahwei/cyber_MITRE_technique_CTI_dataset_v16')\n",
    "\n",
    "# # Split the dataset into train, validation, and test\n",
    "# dataset_split = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# # Further split the train dataset into train and validation\n",
    "# train_dataset, val_dataset = dataset_split['train'].train_test_split(test_size=0.125, seed=42).values()\n",
    "\n",
    "# # Create a DatasetDict containing the splits\n",
    "# dataset_dict = {\n",
    "#     'train': train_dataset,\n",
    "#     'validation': val_dataset,\n",
    "#     'test': dataset_split['test']\n",
    "# }\n",
    "\n",
    "# # Convert to a DatasetDict\n",
    "# dataset = DatasetDict(dataset_dict)\n",
    "\n",
    "# # Print the DatasetDict to get the desired output\n",
    "# print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98e09e3e-8f4b-4537-a669-dd74c797f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import ast\n",
    "\n",
    "\n",
    "def flatten_labels(split):\n",
    "    new_rows = []\n",
    "    for example in split:\n",
    "        lst = ast.literal_eval(example['labels'])\n",
    "        for i in lst:\n",
    "            new_rows.append({'g': example['text1'],'labels': i})\n",
    "    return Dataset.from_list(new_rows)\n",
    "\n",
    "# Apply to all splits\n",
    "dataset = DatasetDict({\n",
    "    'train': flatten_labels(dataset['train']),\n",
    "    'validation': flatten_labels(dataset['validation']),\n",
    "    'test': flatten_labels(dataset['test']),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101f8b2f-2a0e-4491-8a61-b712175bdb57",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cde6f1e2-81e6-4267-97e6-3d5baa724a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inceptionâ€™s malware is modular and the attackers will load plugins based on requirements for each attack. The group has used a range of plugins in recent attacks, some of which are improved versions of plugins used in 2014, while others were previously unseen\n",
      "T1057\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][1000]['text1'])\n",
    "print(dataset['train'][1000]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ca7ce94-f015-459a-bf42-07282ce4e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When communicating with its C2 server, Psylo will use HTTPS with a unique user-agent of (notice the lack of a space between \"5.0\" and \"(Windows\n",
      "T1071.001\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][5]['text1'])\n",
    "print(dataset['train'][5]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b316260-4782-40a7-8448-d04349ce623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The command processing function starts by substituting the main module name and path in the hosting process PEB, with the one of the default internet browser. The path of the main browser of the workstation is obtained by reading the registry value', 'Along the way, HermeticWiperâ€™s more mundane operations provide us with further IOCs to monitor for. These include the momentary creation of the abused driver as well as a system service. It also modifies several registry keys, including setting the SYSTEM\\\\CurrentControlSet\\\\Control\\\\CrashControl CrashDumpEnabled key to 0, effectively disabling crash dumps before the abused driverâ€™s execution starts', 'These Microsoft Office templates are hosted on a command and control server and the downloaded link is embedded in the first stage malicious document', 'Additionally, the IP 211[.]72 [.]242[.]120 is one of the hosts for the domain microsoftmse[.]com, which has been used by several KIVARS variants', 'Additionally, the IP 211[.]72 [.]242[.]120 is one of the hosts for the domain microsoftmse[.]com, which has been used by several KIVARS variants']\n",
      "['T1057', 'T1569.002', 'T1584.004', 'T1056.001', 'T1113']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][:5]['text1'])\n",
    "print(dataset['train'][:5]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2a67699-3b9b-496a-80da-ffd9f5caf82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [(row['text1'], row['labels']) for row in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1475077-7845-419f-a9c9-92dec7188f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The command processing function starts by substituting the main module name and path in the hosting process PEB, with the one of the default internet browser. The path of the main browser of the workstation is obtained by reading the registry value',\n",
       " 'T1057')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef27af-cf92-4834-823b-744b06f64f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd3b8d08-aa4c-4acc-92d9-8a7317e600c9",
   "metadata": {},
   "source": [
    "# 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bc57671-543b-465b-b94c-c9a46c638a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edae202b1aba4f96a3d0ca19e0b9384f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16340 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b755d0d9adc54f949bbe00f53235fbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2882 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd411f6956604e5daec9bb0c38b46d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The command processing function starts by substituting the main module name and path in the hosting process PEB, with the one of the default internet browser. The path of the main browser of the workstation is obtained by reading the registry value\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('ibm-research/CTI-BERT')\n",
    "\n",
    "# Function to tokenize the text data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text1'], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "# Apply tokenization to train and validation datasets\n",
    "train_data = dataset['train'].map(tokenize_function, batched=True)\n",
    "val_data = dataset['validation'].map(tokenize_function, batched=True)\n",
    "test_data = dataset['test'].map(tokenize_function, batched=True)\n",
    "\n",
    "# Print an example to verify\n",
    "print(train_data[0]['text1'])  # It should show tokenized input\n",
    "\n",
    "# # Decode the tokenized text back to human-readable text\n",
    "# decoded_text = tokenizer.decode(train_data[0]['input_ids'], skip_special_tokens=True)\n",
    "# print(f\"Decoded Text: {decoded_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a1d8e17-904c-4be0-aaa3-c59159224364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'command', 'processing', 'function', 'starts', 'by', 'substituting', 'the', 'main', 'module', 'name', 'and', 'path', 'in', 'the', 'hosting', 'process', 'peb', ',', 'with', 'the', 'one', 'of', 'the', 'default', 'internet', 'browser', '.', 'the', 'path', 'of', 'the', 'main', 'browser', 'of', 'the', 'workstation', 'is', 'obtained', 'by', 'reading', 'the', 'registry', 'value', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('ibm-research/CTI-BERT')\n",
    "\n",
    "# Example sentence\n",
    "text = \"The command processing function starts by substituting the main module name and path in the hosting process PEB, with the one of the default internet browser. The path of the main browser of the workstation is obtained by reading the registry value.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c846cd-706d-48cc-8fec-abe6e9cc3277",
   "metadata": {},
   "source": [
    "# 3. Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13415bff-df37-4e6d-955e-940b0b9266ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f14713f6cb4ccbb31563bf90ac1d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16340 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472e42c2e23d4f9eb2451db849802ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2882 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text1': 'The command processing function starts by substituting the main module name and path in the hosting process PEB, with the one of the default internet browser. The path of the main browser of the workstation is obtained by reading the registry value', 'labels': 395, 'input_ids': [2, 114, 979, 3435, 962, 4929, 229, 41301, 114, 1069, 2014, 810, 137, 1508, 120, 114, 4393, 612, 24978, 16, 214, 114, 479, 135, 114, 1074, 904, 1437, 18, 114, 1508, 135, 114, 1069, 1437, 135, 114, 7966, 146, 4395, 229, 2979, 114, 2782, 887, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Extract unique labels (MITRE techniques) from both train and validation datasets\n",
    "labels = list(set(dataset['train']['labels']).union(set(dataset['validation']['labels'])))  # Extract unique labels\n",
    "label_map = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Function to encode labels into integers\n",
    "def encode_labels(examples):\n",
    "    # Safely map the labels, providing a default value if a label is not found in the label_map\n",
    "    examples['labels'] = [label_map.get(label, -1) for label in examples['labels']]\n",
    "    return examples\n",
    "\n",
    "# Apply label encoding\n",
    "train_data = train_data.map(encode_labels, batched=True)\n",
    "val_data = val_data.map(encode_labels, batched=True)\n",
    "\n",
    "# Print an example to verify\n",
    "print(train_data[0])  # It should show tokenized input along with the encoded label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c88c5a4c-5939-4399-a75d-b3ff188fd559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract unique labels (MITRE techniques) and map to integers\n",
    "# labels = list(set(dataset['train']['label']))  # Extract unique labels\n",
    "# label_map = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# # # Extract unique labels (MITRE techniques) from both train and validation datasets\n",
    "# # labels = list(set(dataset['train']['label']).union(set(dataset['validation']['label'])))  # Extract unique labels\n",
    "# # label_map = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# # Function to encode labels into integers\n",
    "# def encode_labels(examples):\n",
    "#     examples['label'] = [label_map[label] for label in examples['label']]\n",
    "#     return examples\n",
    "\n",
    "# # # Function to encode labels into integers\n",
    "# # def encode_labels(examples):\n",
    "# #     for label in examples['label']:\n",
    "# #         if label not in label_map:\n",
    "# #             print(f\"Warning: Label '{label}' not found in label_map!\")  # Optional, for debugging\n",
    "# #     examples['label'] = [label_map.get(label, -1) for label in examples['label']]  # Default to -1 if missing\n",
    "# #     return examples\n",
    "\n",
    "# # Apply label encoding\n",
    "# train_data = train_data.map(encode_labels, batched=True)\n",
    "# val_data = val_data.map(encode_labels, batched=True)\n",
    "\n",
    "# # Print an example to verify\n",
    "# print(train_data[0])  # It should show tokenized input along with the encoded label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba206f8d-5477-483d-acea-d2e0d4233eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract unique labels (MITRE techniques) from both train and validation datasets\n",
    "# labels = list(set(dataset['train']['label']).union(set(dataset['validation']['label'])))  # Extract unique labels\n",
    "# label_map = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6582db5-3464-4138-8349-f3abac95078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# # Load the pre-trained BERT model for sequence classification\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(labels))\n",
    "\n",
    "# # Define the training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          # Output directory\n",
    "#     num_train_epochs=3,              # Number of training epochs\n",
    "#     per_device_train_batch_size=8,   # Batch size for training\n",
    "#     per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "#     evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n",
    "#     # logging_dir='./logs',            # Directory for storing logs\n",
    "# )\n",
    "\n",
    "# # Initialize the Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,                         # The pre-trained model\n",
    "#     args=training_args,                  # The training arguments\n",
    "#     train_dataset=train_data,            # The training dataset\n",
    "#     eval_dataset=val_data,               # The validation dataset\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5016848-0915-454b-91fd-1067ed34070b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d4e712e-de85-4e26-b54d-f85afd69907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    # If using softmax, we need to use argmax to get the final class prediction\n",
    "    preds = preds.argmax(axis=-1)\n",
    "    \n",
    "    # Calculate precision, recall, F1-score, and accuracy\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5510f-eabf-4501-b4b5-a6aa4b46216e",
   "metadata": {},
   "source": [
    "## Train with CTI-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5a17746-bf34-43d2-8898-466bfa18669d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training hyperparameters\\nThe following hyperparameters were used during training:\\n\\nlearning_rate: 0.0005\\ntrain_batch_size: 128\\neval_batch_size: 128\\nseed: 42\\ngradient_accumulation_steps: 16\\ntotal_train_batch_size: 2048\\noptimizer: Adam with betas=(0.9,0.98) and epsilon=1e-06\\nlr_scheduler_type: linear\\nlr_scheduler_warmup_steps: 10000\\ntraining_steps: 200000'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Training hyperparameters\n",
    "The following hyperparameters were used during training:\n",
    "\n",
    "learning_rate: 0.0005\n",
    "train_batch_size: 128\n",
    "eval_batch_size: 128\n",
    "seed: 42\n",
    "gradient_accumulation_steps: 16\n",
    "total_train_batch_size: 2048\n",
    "optimizer: Adam with betas=(0.9,0.98) and epsilon=1e-06\n",
    "lr_scheduler_type: linear\n",
    "lr_scheduler_warmup_steps: 10000\n",
    "training_steps: 200000\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c52cc62f-f859-4108-b21d-17867264078c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ibm-research/CTI-BERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mworamethr\u001b[0m (\u001b[33mworamethr-asian-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter-st124903/projectnlp2025/wandb/run-20250427_051729-wlhsh6ha</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/woramethr-asian-institute-of-technology/huggingface/runs/wlhsh6ha' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/woramethr-asian-institute-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/woramethr-asian-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/woramethr-asian-institute-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/woramethr-asian-institute-of-technology/huggingface/runs/wlhsh6ha' target=\"_blank\">https://wandb.ai/woramethr-asian-institute-of-technology/huggingface/runs/wlhsh6ha</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1533' max='1533' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1533/1533 08:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.868400</td>\n",
       "      <td>2.775704</td>\n",
       "      <td>0.474670</td>\n",
       "      <td>0.371924</td>\n",
       "      <td>0.474670</td>\n",
       "      <td>0.392578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.391900</td>\n",
       "      <td>2.341063</td>\n",
       "      <td>0.527412</td>\n",
       "      <td>0.446690</td>\n",
       "      <td>0.527412</td>\n",
       "      <td>0.462944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.961800</td>\n",
       "      <td>2.228477</td>\n",
       "      <td>0.544414</td>\n",
       "      <td>0.473586</td>\n",
       "      <td>0.544414</td>\n",
       "      <td>0.487865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./final_model1/tokenizer_config.json',\n",
       " './final_model1/special_tokens_map.json',\n",
       " './final_model1/vocab.txt',\n",
       " './final_model1/added_tokens.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('ibm-research/CTI-BERT', num_labels=len(labels))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=8,   \n",
    "    per_device_eval_batch_size=16,   \n",
    "    evaluation_strategy=\"epoch\",     \n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The pre-trained model\n",
    "    args=training_args,                  # The training arguments\n",
    "    train_dataset=train_data,            # The training dataset\n",
    "    eval_dataset=val_data,               # The validation dataset\n",
    "    compute_metrics=compute_metrics      # Add the compute_metrics function\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Optionally save the final model manually (this step is usually not required as the trainer saves it automatically)\n",
    "trainer.save_model(\"./final_model1\")  # You can specify any directory you prefer\n",
    "\n",
    "# Also save the tokenizer (if necessary)\n",
    "tokenizer.save_pretrained(\"./final_model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a54df2d-0c38-42d8-9dd6-57a262613626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='106' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 2.2284772396087646, 'eval_accuracy': 0.5444136016655101, 'eval_precision': 0.47358587962405074, 'eval_recall': 0.5444136016655101, 'eval_f1': 0.4878651706700848, 'eval_runtime': 6.7708, 'eval_samples_per_second': 425.65, 'eval_steps_per_second': 6.794, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter-st124903/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation dataset\n",
    "eval_results = trainer.evaluate(eval_dataset=val_data)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c049fea9-2bbd-406a-ae7e-c67bfaab3f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results: {'eval_runtime': 9.0208, 'eval_samples_per_second': 419.585, 'eval_steps_per_second': 6.651, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Assuming test_data is defined and contains the test set\n",
    "test_results = trainer.evaluate(eval_dataset=test_data)\n",
    "\n",
    "# Print the test results\n",
    "print(\"Test results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc14a15-3f75-46e9-bc8d-859a2e4a96a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22555c-a80c-43ad-9368-da5b56001bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c48139-a8a6-41ca-b995-370ac09303f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82693bfd-1155-4ba4-890f-8fc004cb204e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ef1e5a0-7864-4d6b-a320-10b78a5896fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bfec0e9-ac0a-4f05-af52-4c7b52223fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text for classification:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: T1071\n",
      "Prediction Probabilities: tensor([[2.8728e-04, 6.2711e-04, 4.3520e-04, 3.5924e-03, 7.3747e-04, 3.5778e-03,\n",
      "         4.8376e-04, 2.7944e-03, 3.3418e-04, 2.0135e-04, 1.0493e-03, 7.8652e-03,\n",
      "         1.8842e-03, 1.1182e-04, 6.5573e-05, 3.1861e-03, 2.2755e-04, 1.2578e-04,\n",
      "         1.2454e-04, 1.4929e-04, 9.0229e-05, 4.8688e-03, 3.3605e-04, 5.4051e-04,\n",
      "         4.2112e-04, 4.6200e-04, 1.6454e-03, 5.7851e-04, 1.0574e-04, 2.2906e-04,\n",
      "         3.1178e-04, 4.3422e-03, 1.1435e-04, 3.0856e-04, 6.2763e-05, 2.7954e-04,\n",
      "         9.9990e-05, 1.5496e-03, 6.2040e-03, 9.6692e-04, 8.5517e-05, 1.1505e-03,\n",
      "         8.7555e-04, 3.4619e-04, 2.0657e-03, 4.6954e-03, 1.1256e-04, 3.1992e-03,\n",
      "         3.7916e-04, 2.8691e-04, 1.5573e-04, 2.8532e-04, 5.9052e-04, 1.4731e-03,\n",
      "         4.9593e-03, 1.0899e-04, 1.1049e-03, 6.6293e-04, 1.7441e-04, 8.5080e-05,\n",
      "         2.5543e-04, 5.9926e-04, 9.7129e-05, 1.4882e-02, 9.6313e-05, 9.9160e-05,\n",
      "         2.8374e-04, 9.3408e-05, 3.4460e-04, 2.2904e-03, 4.9148e-05, 4.8779e-04,\n",
      "         3.4919e-03, 4.7590e-04, 2.6704e-04, 4.2219e-04, 3.7298e-04, 2.9079e-03,\n",
      "         5.6730e-02, 1.3571e-04, 3.4976e-04, 2.2023e-04, 8.8266e-04, 7.6022e-05,\n",
      "         1.5666e-03, 7.1354e-04, 2.3492e-03, 1.6563e-04, 1.7672e-04, 6.1421e-04,\n",
      "         1.7580e-03, 8.2717e-05, 2.0382e-03, 5.2882e-04, 6.3563e-03, 1.2247e-04,\n",
      "         1.0468e-03, 3.2880e-04, 5.2017e-04, 1.2392e-04, 6.8107e-05, 5.6602e-04,\n",
      "         6.7694e-03, 1.2056e-03, 3.5110e-04, 4.0578e-03, 3.4293e-03, 2.7891e-03,\n",
      "         3.6918e-04, 1.8252e-04, 2.1457e-04, 9.9128e-03, 3.1720e-04, 8.1094e-05,\n",
      "         5.1264e-04, 3.4159e-04, 8.6821e-05, 6.2092e-04, 3.1548e-04, 1.9326e-04,\n",
      "         1.9115e-03, 1.5983e-03, 2.1086e-03, 1.3906e-04, 4.1169e-04, 1.4669e-04,\n",
      "         1.5760e-04, 1.1389e-03, 7.7407e-04, 4.0962e-03, 7.9444e-05, 5.7719e-05,\n",
      "         3.2009e-04, 1.0235e-04, 1.3952e-04, 1.0711e-04, 2.6159e-03, 1.4801e-03,\n",
      "         2.2217e-04, 6.8813e-05, 3.2077e-03, 1.6612e-03, 3.1983e-04, 1.3425e-02,\n",
      "         1.5895e-04, 2.8220e-04, 2.7682e-03, 3.6190e-04, 3.6703e-04, 2.5556e-04,\n",
      "         1.9248e-02, 9.1967e-04, 2.9752e-03, 9.0430e-05, 1.7122e-04, 1.1692e-04,\n",
      "         5.6186e-04, 4.0645e-04, 6.3585e-03, 1.6154e-03, 4.9248e-04, 1.1350e-04,\n",
      "         7.5558e-04, 3.5789e-04, 8.3563e-05, 2.6694e-04, 3.0167e-04, 3.2017e-04,\n",
      "         1.0493e-03, 3.0837e-04, 1.9768e-04, 3.3691e-04, 1.9857e-03, 4.3774e-04,\n",
      "         2.7299e-03, 1.4619e-04, 2.6098e-04, 4.6715e-03, 4.2472e-04, 8.9350e-05,\n",
      "         1.0234e-03, 3.3384e-04, 3.0158e-04, 2.4168e-04, 6.3452e-05, 1.0419e-04,\n",
      "         3.7762e-04, 1.5442e-04, 4.9879e-04, 2.1177e-04, 9.4820e-04, 2.8595e-03,\n",
      "         2.7431e-04, 8.8991e-03, 1.2256e-04, 6.4134e-05, 8.2656e-04, 1.9334e-04,\n",
      "         1.9503e-04, 1.2128e-04, 2.2580e-02, 1.7885e-04, 1.8658e-03, 4.4141e-03,\n",
      "         5.8722e-03, 3.2555e-04, 4.1070e-04, 1.5279e-04, 7.8919e-04, 7.9253e-05,\n",
      "         4.0257e-04, 1.3905e-04, 3.5063e-04, 4.8316e-04, 3.0503e-03, 8.2210e-05,\n",
      "         1.1965e-03, 6.3739e-05, 3.8711e-03, 1.4797e-03, 1.8175e-03, 2.5041e-03,\n",
      "         1.3327e-04, 3.2626e-04, 1.6570e-03, 1.3344e-02, 1.6318e-04, 1.9197e-03,\n",
      "         1.2027e-02, 2.4760e-03, 2.4365e-04, 2.8582e-04, 2.7143e-04, 1.1342e-03,\n",
      "         5.2183e-04, 6.4563e-04, 6.7668e-05, 8.9900e-04, 3.4104e-04, 1.3551e-04,\n",
      "         2.2447e-04, 1.0602e-04, 5.3302e-04, 3.1835e-02, 2.6274e-04, 1.9429e-04,\n",
      "         6.4155e-04, 8.6168e-03, 5.2238e-04, 3.7692e-04, 3.2163e-04, 5.3450e-03,\n",
      "         6.4251e-05, 9.0205e-05, 3.5633e-03, 3.1502e-03, 2.8377e-04, 4.5688e-04,\n",
      "         2.6950e-04, 5.4070e-04, 1.3584e-03, 3.3606e-04, 5.9324e-03, 4.5761e-04,\n",
      "         3.2616e-04, 2.7315e-04, 9.6324e-04, 1.3051e-04, 3.3133e-03, 1.4007e-04,\n",
      "         3.4424e-03, 4.4260e-04, 2.0083e-03, 2.4947e-04, 4.5639e-04, 3.9089e-04,\n",
      "         2.0143e-04, 6.3011e-04, 4.2260e-03, 1.5842e-04, 1.1287e-04, 5.4226e-04,\n",
      "         5.1672e-04, 1.5073e-03, 6.5382e-04, 1.0847e-04, 1.2629e-03, 2.1337e-03,\n",
      "         2.7194e-04, 2.8202e-04, 3.4440e-04, 5.1570e-04, 4.6055e-04, 1.3389e-03,\n",
      "         1.2692e-04, 8.2170e-05, 2.2094e-04, 4.6330e-04, 2.7574e-04, 1.5665e-04,\n",
      "         7.5121e-05, 2.6721e-04, 6.4253e-04, 3.1737e-04, 6.9786e-04, 4.9657e-04,\n",
      "         4.6274e-04, 4.6160e-04, 2.5779e-02, 6.9447e-04, 1.3942e-03, 2.7449e-04,\n",
      "         7.1431e-04, 1.5398e-04, 1.2912e-04, 6.5788e-04, 1.5778e-03, 6.6984e-04,\n",
      "         5.8335e-04, 1.4632e-04, 2.8347e-04, 1.5979e-02, 5.6129e-04, 1.7869e-02,\n",
      "         3.1642e-03, 8.7486e-05, 9.9201e-04, 6.1742e-04, 1.6136e-03, 3.7775e-04,\n",
      "         2.0515e-04, 5.7654e-04, 1.4700e-03, 4.2905e-03, 7.7876e-05, 2.9904e-04,\n",
      "         3.2995e-04, 4.1345e-03, 1.6341e-04, 2.0430e-03, 1.3474e-04, 1.2497e-04,\n",
      "         3.9184e-04, 1.6463e-04, 6.6939e-04, 1.0480e-04, 1.8903e-03, 9.5122e-05,\n",
      "         2.9811e-04, 2.8339e-04, 5.7800e-03, 6.4704e-02, 1.1716e-04, 6.7515e-04,\n",
      "         1.9778e-04, 8.0051e-05, 1.5278e-03, 5.6714e-05, 1.6909e-03, 1.0459e-04,\n",
      "         1.2301e-03, 3.3945e-04, 4.4130e-04, 2.3884e-04, 5.3011e-05, 1.2016e-04,\n",
      "         2.3503e-04, 5.3305e-04, 1.6427e-04, 1.0754e-04, 1.2771e-03, 6.7592e-05,\n",
      "         2.6398e-04, 2.9079e-04, 1.5421e-04, 2.1442e-04, 7.3056e-03, 2.1936e-03,\n",
      "         2.8315e-04, 3.2049e-04, 9.1280e-05, 1.1083e-04, 2.6818e-04, 2.1722e-03,\n",
      "         9.2814e-05, 1.0201e-04, 1.7682e-03, 2.3728e-04, 7.2180e-04, 8.8743e-05,\n",
      "         6.5262e-03, 1.0799e-04, 8.0371e-03, 4.2397e-03, 6.6879e-03, 1.4884e-02,\n",
      "         7.8178e-04, 1.3319e-04, 6.4181e-04, 2.5425e-04, 7.9088e-04, 6.9789e-03,\n",
      "         1.9423e-04, 1.7655e-04, 4.2095e-04, 2.6942e-04, 3.3647e-04, 3.8687e-04,\n",
      "         2.6568e-03, 1.8103e-03, 7.5262e-04, 1.4355e-03, 1.7818e-02, 1.1689e-04,\n",
      "         3.6255e-04, 1.4827e-04, 3.0438e-04, 3.2789e-03, 7.6866e-04, 1.0924e-04,\n",
      "         1.8119e-02, 4.8703e-04, 5.8601e-04, 7.4103e-04, 6.5020e-04, 3.0308e-03,\n",
      "         1.2120e-03, 2.1333e-04, 1.1648e-04, 2.4318e-04, 1.4884e-04, 2.7874e-04,\n",
      "         1.1608e-04, 1.2136e-04, 1.3725e-04, 7.8420e-04, 1.5937e-04, 1.8058e-03,\n",
      "         1.9133e-02, 9.6966e-03, 7.5694e-04, 2.3586e-04, 1.4623e-02, 1.0974e-04,\n",
      "         3.8330e-05, 4.5838e-04, 4.4646e-03, 1.7695e-04, 4.7772e-04, 3.9868e-02,\n",
      "         1.1474e-03, 2.4406e-04, 1.0897e-03, 1.0921e-03, 3.3526e-04, 4.4951e-03,\n",
      "         9.4225e-05, 1.3870e-03, 1.2173e-04, 6.1012e-04, 3.2487e-04, 4.0809e-04,\n",
      "         1.1923e-04, 7.6429e-05, 3.4505e-04, 2.4127e-02, 1.3207e-03, 9.4956e-04,\n",
      "         8.3196e-05, 5.6896e-04, 1.4428e-03, 1.5816e-04, 2.2249e-04, 1.3069e-03,\n",
      "         1.3456e-02, 5.7933e-03, 1.7393e-04, 2.4937e-03, 6.5817e-04, 5.9620e-05,\n",
      "         4.3218e-04, 2.3724e-03, 5.3507e-03, 3.1438e-03, 3.8896e-04, 1.0083e-04,\n",
      "         1.6105e-04, 6.1816e-05, 4.6834e-04, 1.7497e-03, 1.9540e-03, 9.6776e-04,\n",
      "         1.2598e-04, 6.3669e-04, 1.3052e-04, 1.4581e-03, 4.1050e-04, 1.9222e-04,\n",
      "         1.8271e-04, 2.1209e-04, 1.2872e-03, 4.6452e-04, 1.5848e-04, 1.8516e-04,\n",
      "         1.7775e-04, 8.6794e-04, 2.1889e-04, 3.6051e-03, 1.5553e-02]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: ['T1203']\n",
      "Prediction Probabilities: tensor([[8.5942e-05, 1.7216e-04, 2.1037e-04,  ..., 4.3844e-04, 1.2077e-03,\n",
      "         1.0486e-03]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./final_model1')\n",
    "# model = BertForSequenceClassification.from_pretrained('ibm-research/CTI-BERT')\n",
    "tokenizer = BertTokenizer.from_pretrained('ibm-research/CTI-BERT')\n",
    "\n",
    "# Function to get prediction for manual input text\n",
    "def predict(input_text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Get the model's output\n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the logits (model output before applying softmax)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Apply softmax to get probabilities (for multi-class classification)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the predicted label (index of max probability)\n",
    "    predicted_label = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    return predicted_label, probabilities\n",
    "\n",
    "# Test with manual input\n",
    "input_text = input(\"Enter text for classification: \")\n",
    "\n",
    "predicted_label, probabilities = predict(input_text)\n",
    "\n",
    "# Assuming `labels` contains the label names and was created previously (e.g., from `dataset['train']['label']`)\n",
    "# You can map the predicted label index back to the label name\n",
    "predicted_label_name = labels[predicted_label]\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label_name}\")\n",
    "print(f\"Prediction Probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6541db21-437b-4bfc-91e0-2b5bf1452952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/jupyter-\n",
      "[nltk_data]     st124903/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "An Iranian state-sponsored actor has been observed scanning and attempting to abuse the Log4Shell flaw in publicly-exposed Java applications to deploy a hitherto undocumented PowerShell-based modular backdoor dubbed \"CharmPower\" for follow-on post-exploitation.\n",
      "Predicted Label: T1190\n",
      "Prediction Probabilities: tensor([[5.1243e-04, 8.1736e-04, 1.9886e-03, 6.2065e-04, 5.7447e-04, 1.0098e-02,\n",
      "         8.5801e-04, 1.2725e-03, 1.2196e-03, 7.8081e-04, 1.6700e-03, 1.7396e-03,\n",
      "         9.5309e-04, 6.1896e-04, 5.0379e-04, 9.5596e-04, 5.8830e-04, 8.1963e-04,\n",
      "         4.7799e-04, 8.1372e-04, 7.2214e-04, 1.5212e-03, 5.9348e-04, 1.8004e-03,\n",
      "         8.8915e-04, 9.3078e-04, 1.5904e-03, 1.3662e-03, 2.5677e-04, 6.9799e-04,\n",
      "         2.1121e-03, 1.4003e-03, 5.5481e-04, 9.2005e-04, 3.9183e-04, 9.1157e-04,\n",
      "         7.2661e-04, 6.3973e-04, 5.8083e-04, 1.6261e-03, 3.4794e-04, 7.4668e-04,\n",
      "         5.6557e-04, 5.8662e-04, 1.4305e-03, 1.1343e-03, 7.2599e-04, 3.5753e-03,\n",
      "         3.4277e-04, 1.0854e-03, 6.3097e-04, 8.2120e-04, 9.9269e-04, 7.8947e-04,\n",
      "         2.8406e-03, 4.7506e-04, 6.0557e-04, 1.6740e-03, 7.7277e-04, 5.1364e-04,\n",
      "         1.0923e-03, 8.1990e-04, 4.9303e-04, 2.3848e-03, 8.8856e-04, 5.2135e-04,\n",
      "         1.0646e-03, 4.1800e-04, 6.7472e-04, 3.7980e-04, 3.4286e-04, 5.0164e-04,\n",
      "         9.0453e-04, 1.0751e-03, 9.7602e-04, 1.4151e-03, 1.5825e-03, 7.5775e-04,\n",
      "         3.5918e-03, 6.6382e-04, 6.0247e-04, 7.2050e-04, 7.8343e-04, 7.3652e-04,\n",
      "         2.1696e-03, 9.4258e-04, 4.7391e-04, 4.2502e-04, 4.5233e-04, 5.6211e-04,\n",
      "         1.2448e-03, 4.5156e-04, 1.1048e-03, 9.5195e-04, 2.3674e-03, 3.9987e-04,\n",
      "         7.3781e-04, 7.9430e-04, 4.6063e-04, 7.9745e-04, 4.3674e-04, 9.8183e-04,\n",
      "         1.0036e-03, 5.1707e-04, 1.1207e-03, 1.8642e-03, 1.5785e-03, 2.2707e-03,\n",
      "         6.2485e-04, 9.1143e-04, 5.0937e-04, 3.2911e-04, 1.3800e-03, 5.0894e-04,\n",
      "         1.7210e-03, 9.6190e-04, 3.0432e-04, 7.4522e-04, 7.5083e-04, 8.4669e-04,\n",
      "         6.2444e-04, 7.9125e-04, 7.7421e-04, 6.0314e-04, 6.3758e-04, 7.8981e-04,\n",
      "         5.0465e-04, 8.1845e-04, 1.3001e-03, 3.5372e-03, 6.4596e-04, 4.4983e-04,\n",
      "         1.4211e-03, 5.3132e-04, 1.1013e-03, 5.3540e-04, 1.7902e-03, 1.3951e-03,\n",
      "         9.1982e-04, 2.4836e-04, 2.1906e-03, 9.9300e-04, 6.0863e-04, 1.6525e-03,\n",
      "         6.0721e-04, 5.5797e-04, 1.5870e-03, 7.0096e-04, 8.4636e-04, 9.5003e-04,\n",
      "         2.6727e-03, 8.1710e-04, 1.4676e-03, 6.6669e-04, 7.3291e-04, 6.3375e-04,\n",
      "         1.3052e-03, 9.4065e-04, 1.7072e-03, 1.2044e-03, 5.4950e-04, 6.0376e-04,\n",
      "         1.0182e-03, 6.5832e-04, 6.0660e-04, 1.3168e-03, 7.4784e-04, 5.1947e-04,\n",
      "         6.2563e-04, 3.5392e-04, 5.5044e-04, 9.0647e-04, 1.5003e-03, 1.0118e-03,\n",
      "         1.0640e-03, 7.7854e-04, 5.3642e-04, 1.7772e-03, 1.0182e-03, 3.9095e-04,\n",
      "         1.6100e-03, 1.2654e-03, 1.3693e-03, 9.3447e-04, 3.3893e-04, 3.6751e-04,\n",
      "         4.5917e-04, 5.3248e-04, 1.2397e-03, 6.7460e-04, 1.2231e-03, 8.8187e-04,\n",
      "         7.7462e-04, 6.0208e-04, 7.4840e-04, 5.4427e-04, 8.3332e-04, 4.3197e-04,\n",
      "         4.3367e-04, 5.6313e-04, 2.3886e-03, 3.6888e-04, 9.7839e-04, 1.0184e-03,\n",
      "         4.7828e-04, 1.1816e-03, 1.1277e-03, 1.5930e-03, 9.5645e-04, 4.3940e-04,\n",
      "         9.8370e-04, 5.7964e-04, 1.0838e-03, 7.6799e-04, 2.4778e-03, 3.4629e-04,\n",
      "         8.1206e-04, 2.4215e-04, 1.4674e-03, 3.1474e-03, 2.4425e-03, 1.7653e-03,\n",
      "         1.1160e-03, 5.6634e-04, 1.5964e-03, 2.2174e-03, 7.4174e-04, 9.3532e-04,\n",
      "         7.3771e-04, 1.1701e-03, 1.0820e-03, 1.0261e-03, 9.1649e-04, 8.7688e-04,\n",
      "         6.9612e-04, 6.8039e-04, 5.4766e-04, 7.1697e-04, 5.4071e-04, 9.9685e-04,\n",
      "         6.2672e-04, 7.1613e-04, 5.4243e-04, 1.9587e-03, 8.0976e-04, 6.3086e-04,\n",
      "         9.8227e-04, 2.6479e-03, 9.8493e-04, 1.0197e-03, 8.1711e-04, 3.8516e-04,\n",
      "         4.9008e-04, 3.9403e-04, 1.3583e-03, 1.0985e-03, 1.0062e-03, 7.5188e-04,\n",
      "         1.3995e-03, 9.8327e-04, 6.9101e-04, 1.3998e-03, 1.5199e-03, 1.1598e-03,\n",
      "         1.1385e-03, 4.9174e-04, 8.2618e-04, 1.1082e-03, 2.3803e-03, 4.9122e-04,\n",
      "         1.3056e-03, 8.0618e-04, 8.0578e-04, 5.5699e-04, 1.5431e-03, 1.1172e-03,\n",
      "         8.8841e-04, 8.0430e-04, 1.4020e-03, 3.4177e-04, 1.1613e-03, 1.1185e-03,\n",
      "         5.2509e-03, 1.2700e-03, 1.5479e-03, 4.6240e-04, 2.7439e-02, 6.4221e-04,\n",
      "         5.6731e-04, 1.6803e-03, 4.0586e-04, 1.4167e-03, 9.6965e-04, 7.6690e-04,\n",
      "         5.3787e-04, 4.7208e-04, 1.1300e-03, 1.3031e-03, 4.2672e-04, 7.5806e-04,\n",
      "         4.8877e-04, 5.7150e-04, 1.0753e-03, 1.0643e-03, 1.4725e-03, 5.9935e-04,\n",
      "         1.1324e-03, 5.4330e-04, 9.3166e-04, 8.8199e-04, 1.2781e-03, 7.9710e-04,\n",
      "         3.4545e-04, 5.8507e-04, 6.9614e-04, 8.3752e-04, 1.5376e-03, 6.0415e-04,\n",
      "         1.0500e-03, 8.2090e-04, 8.9977e-04, 2.6797e-03, 5.9224e-04, 3.0639e-03,\n",
      "         1.0547e-03, 6.7370e-04, 1.0782e-03, 1.7838e-03, 9.3115e-04, 9.6521e-04,\n",
      "         1.2197e-03, 5.7681e-04, 1.8918e-03, 1.0899e-03, 6.6093e-04, 7.9161e-04,\n",
      "         8.5509e-04, 3.2305e-04, 5.2389e-04, 2.8866e-03, 8.1623e-04, 9.8354e-04,\n",
      "         1.0119e-03, 8.3424e-04, 3.4408e-04, 3.4070e-04, 7.2469e-04, 5.4851e-04,\n",
      "         7.6138e-04, 5.6089e-04, 2.0153e-03, 8.9098e-04, 6.3995e-04, 1.1530e-03,\n",
      "         7.2014e-04, 9.5478e-04, 6.3742e-04, 4.4482e-04, 1.1085e-03, 7.0640e-04,\n",
      "         6.5187e-02, 8.2299e-04, 7.7831e-04, 5.8598e-04, 4.8181e-04, 6.9388e-04,\n",
      "         8.3626e-04, 7.8130e-04, 5.8227e-04, 6.5210e-04, 1.5648e-03, 8.7877e-04,\n",
      "         6.4324e-04, 8.7134e-04, 5.3562e-04, 1.1821e-03, 1.2837e-03, 5.0449e-03,\n",
      "         6.8156e-04, 4.4907e-04, 2.5478e-04, 4.6732e-04, 6.3125e-04, 1.3079e-03,\n",
      "         6.2251e-04, 3.4519e-04, 4.5126e-03, 4.4516e-04, 1.8245e-03, 1.2641e-03,\n",
      "         1.6772e-03, 7.5292e-04, 6.7760e-04, 1.5597e-03, 1.4807e-03, 6.0726e-04,\n",
      "         4.9301e-04, 6.6333e-04, 1.4092e-03, 8.0118e-04, 1.4755e-03, 2.1295e-03,\n",
      "         5.2061e-04, 1.4736e-03, 9.7616e-03, 1.1445e-03, 1.2963e-03, 7.2706e-04,\n",
      "         1.2381e-03, 1.1816e-03, 1.5545e-03, 1.2844e-03, 1.2931e-03, 7.9280e-04,\n",
      "         7.8399e-04, 4.3629e-04, 1.9195e-03, 1.3654e-03, 1.3378e-03, 3.0695e-04,\n",
      "         8.6873e-04, 1.2769e-03, 1.1099e-03, 3.6963e-04, 1.7085e-03, 8.8898e-04,\n",
      "         6.2310e-04, 9.8690e-04, 1.3942e-03, 8.7356e-04, 6.0690e-04, 4.1897e-04,\n",
      "         9.2194e-04, 3.3385e-04, 5.9560e-04, 9.5969e-04, 1.3536e-03, 4.6891e-04,\n",
      "         1.5969e-03, 1.4391e-03, 1.4306e-03, 9.7905e-04, 1.3592e-03, 5.8313e-04,\n",
      "         4.6881e-04, 5.8981e-04, 1.4116e-03, 6.6957e-04, 1.3278e-03, 1.6781e-03,\n",
      "         1.0665e-03, 5.3685e-04, 3.3573e-01, 1.5986e-03, 6.2589e-04, 6.1272e-04,\n",
      "         4.2267e-04, 1.0364e-03, 5.2023e-04, 1.5465e-03, 5.6832e-04, 8.1431e-04,\n",
      "         5.1207e-04, 6.0911e-04, 1.0685e-03, 8.9923e-04, 1.3330e-03, 6.8612e-04,\n",
      "         5.0570e-04, 6.7420e-04, 9.4479e-04, 8.7794e-04, 7.7393e-04, 3.4351e-03,\n",
      "         1.3035e-03, 5.5148e-03, 7.8224e-04, 3.3190e-02, 1.3089e-03, 3.7899e-04,\n",
      "         1.6999e-03, 1.1523e-03, 8.2234e-04, 1.5998e-02, 5.6206e-04, 5.1467e-04,\n",
      "         5.0084e-04, 5.1946e-04, 1.1729e-03, 1.2059e-03, 1.0861e-03, 1.1384e-03,\n",
      "         7.4544e-04, 1.9657e-03, 8.0707e-04, 5.6829e-04, 5.0728e-04, 6.8754e-04,\n",
      "         7.9362e-04, 5.7924e-04, 5.4745e-04, 8.7257e-04, 7.5891e-04, 7.4350e-04,\n",
      "         8.4959e-04, 1.1139e-03, 4.9686e-04, 1.1441e-03, 8.6004e-04]])\n",
      "\n",
      "Sentence: \"The actor's attack setup was obviously rushed, as they used the basic open-source tool for the exploitation and based their operations on previous infrastructure, which made the attack easier to detect and attribute,\" researchers from Check Point said in a report published this week.\n",
      "Predicted Label: T1588.002\n",
      "Prediction Probabilities: tensor([[2.4026e-04, 5.1910e-04, 1.0565e-03, 3.9828e-03, 3.1243e-04, 9.5403e-03,\n",
      "         1.4583e-03, 2.1606e-03, 6.1456e-04, 2.3332e-04, 1.2460e-03, 8.7432e-03,\n",
      "         1.7473e-03, 1.3241e-04, 6.5530e-05, 1.5928e-03, 1.9085e-04, 1.5920e-04,\n",
      "         1.3796e-04, 1.6315e-04, 1.0679e-04, 3.3029e-03, 2.0347e-04, 5.4766e-04,\n",
      "         4.0755e-04, 9.6830e-04, 2.0340e-03, 6.6337e-04, 6.8181e-05, 2.3092e-04,\n",
      "         3.6159e-04, 4.2326e-03, 2.0716e-04, 5.8377e-04, 8.3595e-05, 1.7467e-04,\n",
      "         1.2474e-04, 9.7990e-04, 2.7379e-03, 1.0200e-03, 8.9916e-05, 6.6498e-04,\n",
      "         5.2855e-04, 3.2648e-04, 1.2466e-03, 3.1268e-03, 1.7028e-04, 1.9809e-03,\n",
      "         3.1151e-04, 2.2085e-04, 1.7249e-04, 2.5977e-04, 7.6367e-04, 3.1097e-03,\n",
      "         1.0078e-01, 1.3100e-04, 7.1032e-04, 6.5931e-04, 2.7081e-04, 8.5142e-05,\n",
      "         3.9628e-04, 6.4399e-04, 7.0504e-05, 1.7035e-02, 1.4661e-04, 8.8866e-05,\n",
      "         3.0256e-04, 9.1881e-05, 4.7209e-04, 1.1429e-03, 3.6253e-05, 8.7079e-04,\n",
      "         1.1748e-03, 5.0208e-04, 3.2326e-04, 1.0244e-03, 4.2132e-04, 2.8532e-03,\n",
      "         3.6741e-02, 1.6451e-04, 2.6619e-04, 2.7718e-04, 9.3802e-04, 1.1084e-04,\n",
      "         1.2758e-03, 7.6585e-04, 2.3437e-03, 3.0271e-04, 1.0860e-04, 3.0686e-04,\n",
      "         5.6236e-03, 9.4120e-05, 2.1701e-03, 4.1997e-04, 2.6162e-02, 8.0173e-05,\n",
      "         1.2530e-03, 4.7745e-04, 3.8465e-04, 1.3472e-04, 7.8339e-05, 5.9386e-04,\n",
      "         8.7709e-03, 6.7637e-04, 5.9147e-04, 4.0978e-03, 4.3848e-03, 3.1085e-03,\n",
      "         5.1444e-04, 1.4336e-04, 2.6197e-04, 9.4310e-03, 6.2556e-04, 8.8702e-05,\n",
      "         5.8306e-04, 3.1878e-04, 9.2933e-05, 9.4693e-04, 1.8189e-04, 2.5677e-04,\n",
      "         1.1603e-03, 1.5813e-03, 1.0241e-03, 1.2494e-04, 3.5095e-04, 1.5088e-04,\n",
      "         1.7983e-04, 7.1352e-04, 1.5817e-03, 2.3700e-02, 7.7513e-05, 4.7811e-05,\n",
      "         5.0001e-04, 1.4514e-04, 2.1813e-04, 1.4517e-04, 2.0661e-03, 1.5516e-03,\n",
      "         1.8178e-04, 6.4570e-05, 1.7783e-03, 1.8435e-03, 2.6321e-04, 1.2203e-02,\n",
      "         1.9562e-04, 2.9861e-04, 1.0885e-02, 3.2522e-04, 4.0642e-04, 3.2208e-04,\n",
      "         5.5042e-03, 1.2759e-03, 2.3183e-03, 1.3984e-04, 1.3893e-04, 1.2893e-04,\n",
      "         6.8938e-04, 5.8264e-04, 7.3926e-03, 3.1725e-03, 2.8457e-04, 1.2948e-04,\n",
      "         1.2270e-03, 3.4374e-04, 9.4419e-05, 4.3198e-04, 1.7541e-04, 2.2665e-04,\n",
      "         1.2255e-03, 1.1611e-04, 1.7912e-04, 2.6852e-04, 5.2183e-03, 1.0248e-03,\n",
      "         3.0007e-03, 1.7742e-04, 2.0496e-04, 3.4823e-03, 5.1666e-04, 1.1372e-04,\n",
      "         1.0149e-03, 4.5606e-04, 5.4207e-04, 1.4427e-04, 6.5734e-05, 1.0935e-04,\n",
      "         4.6912e-04, 2.0663e-04, 1.2845e-03, 2.7281e-04, 1.0719e-03, 1.9857e-03,\n",
      "         3.6559e-04, 9.0161e-03, 1.2390e-04, 6.4467e-05, 8.0203e-04, 1.3944e-04,\n",
      "         1.3935e-04, 1.9699e-04, 7.9364e-03, 1.5402e-04, 8.9187e-04, 2.9042e-03,\n",
      "         3.8214e-03, 5.9849e-04, 4.1867e-04, 3.0117e-04, 7.9174e-04, 8.4080e-05,\n",
      "         4.5575e-04, 2.5009e-04, 2.7350e-04, 5.6306e-04, 2.6565e-03, 6.9600e-05,\n",
      "         2.6386e-03, 5.1799e-05, 1.5346e-03, 3.4519e-03, 4.1343e-03, 1.3224e-03,\n",
      "         1.7942e-04, 2.5841e-04, 2.0664e-03, 2.6728e-02, 2.0794e-04, 1.2038e-03,\n",
      "         4.2139e-03, 1.5646e-03, 1.9407e-04, 3.1608e-04, 4.0538e-04, 1.4134e-03,\n",
      "         5.8435e-04, 1.0074e-03, 1.2200e-04, 7.6251e-04, 5.8315e-04, 2.3613e-04,\n",
      "         1.9529e-04, 1.3202e-04, 7.0461e-04, 1.4001e-02, 2.0203e-04, 3.9996e-04,\n",
      "         5.4822e-04, 1.0885e-02, 7.4280e-04, 5.1051e-04, 3.6773e-04, 1.9855e-03,\n",
      "         5.7470e-05, 5.9309e-05, 1.5101e-03, 2.3074e-03, 2.4175e-04, 8.7756e-04,\n",
      "         4.2923e-04, 5.1171e-04, 2.2404e-03, 4.1949e-04, 4.2347e-03, 7.8590e-04,\n",
      "         4.4712e-04, 2.1370e-04, 1.1643e-03, 1.5111e-04, 1.5197e-02, 2.0587e-04,\n",
      "         2.4382e-03, 3.4363e-04, 1.1481e-03, 2.0883e-04, 6.8906e-04, 5.6662e-04,\n",
      "         2.9727e-04, 5.3280e-04, 4.1264e-03, 1.6398e-04, 1.3065e-04, 6.4269e-04,\n",
      "         8.3028e-04, 1.4273e-03, 9.9006e-04, 1.1118e-04, 1.5129e-03, 3.7069e-03,\n",
      "         3.3779e-04, 2.8666e-04, 4.7855e-04, 9.6999e-04, 4.9411e-04, 1.6530e-03,\n",
      "         1.5214e-04, 1.2687e-04, 2.2756e-04, 4.1257e-04, 2.7393e-04, 1.8741e-04,\n",
      "         9.4537e-05, 3.3823e-04, 4.6645e-04, 2.8541e-04, 4.4892e-04, 5.6196e-04,\n",
      "         4.3319e-04, 6.8703e-04, 6.4178e-03, 9.1687e-04, 1.5492e-03, 2.1402e-04,\n",
      "         2.6273e-04, 2.3794e-04, 1.5324e-04, 9.7846e-04, 8.5019e-04, 4.5168e-04,\n",
      "         5.4319e-04, 1.6334e-04, 4.5069e-04, 1.4055e-02, 6.3419e-04, 3.3031e-02,\n",
      "         1.3551e-03, 1.0216e-04, 2.7624e-03, 7.7924e-04, 1.7903e-03, 4.3717e-04,\n",
      "         2.1390e-04, 6.6195e-04, 1.6589e-03, 7.2836e-03, 1.3044e-04, 3.3176e-04,\n",
      "         4.1294e-04, 3.8183e-03, 1.8784e-04, 2.2245e-03, 1.0911e-04, 1.3449e-04,\n",
      "         6.0990e-04, 2.1711e-04, 4.8498e-04, 9.3988e-05, 2.9922e-03, 1.1270e-04,\n",
      "         2.8642e-04, 2.7978e-04, 2.9463e-03, 3.8413e-03, 2.1582e-04, 9.7263e-04,\n",
      "         2.4733e-04, 1.0990e-04, 1.0618e-03, 1.0543e-04, 2.0182e-03, 1.7023e-04,\n",
      "         3.3968e-03, 3.4204e-04, 5.1352e-04, 2.9447e-04, 6.2496e-05, 1.2483e-04,\n",
      "         2.9744e-04, 4.1403e-04, 1.4595e-04, 1.6609e-04, 2.1250e-03, 1.6204e-04,\n",
      "         3.0578e-04, 5.6820e-04, 2.1681e-04, 3.1147e-04, 2.2899e-03, 4.2373e-03,\n",
      "         2.9807e-04, 2.6510e-04, 7.3791e-05, 8.2560e-05, 2.8295e-04, 2.5970e-03,\n",
      "         1.1428e-04, 9.7934e-05, 3.5565e-03, 2.5877e-04, 1.2696e-03, 1.4290e-04,\n",
      "         5.6618e-03, 1.5609e-04, 3.5549e-03, 1.8087e-03, 3.8238e-03, 1.0876e-02,\n",
      "         1.2199e-03, 1.9764e-04, 1.3570e-03, 3.6980e-04, 5.0356e-04, 8.9287e-03,\n",
      "         2.2475e-04, 2.4935e-04, 1.0981e-03, 3.0170e-04, 2.6413e-04, 4.4695e-04,\n",
      "         1.2187e-03, 9.8062e-04, 8.7889e-04, 1.8297e-03, 7.4873e-03, 1.7720e-04,\n",
      "         4.0209e-04, 8.7607e-05, 4.9188e-04, 2.7274e-03, 1.0781e-03, 8.1036e-05,\n",
      "         1.8310e-03, 8.4195e-04, 6.8700e-04, 8.2720e-04, 7.8465e-04, 4.3940e-03,\n",
      "         8.2270e-04, 3.5265e-04, 1.5848e-04, 2.2749e-04, 3.0721e-04, 3.0095e-04,\n",
      "         2.6518e-04, 1.5566e-04, 1.1676e-04, 1.1934e-03, 1.5246e-04, 6.8646e-04,\n",
      "         2.9613e-03, 1.0051e-02, 5.2783e-04, 3.6031e-04, 3.3777e-02, 1.1629e-04,\n",
      "         4.5091e-05, 5.0612e-04, 3.6486e-03, 3.9205e-04, 4.3716e-04, 2.9115e-03,\n",
      "         1.5039e-03, 3.0734e-04, 2.9559e-03, 1.3820e-03, 3.3793e-04, 8.8336e-04,\n",
      "         1.4710e-04, 2.1352e-03, 1.3657e-04, 8.1542e-04, 3.8085e-04, 3.7619e-04,\n",
      "         1.0723e-04, 1.5684e-04, 4.5717e-04, 5.9641e-02, 1.1574e-03, 7.8022e-04,\n",
      "         1.1535e-04, 5.1466e-04, 2.1573e-03, 1.3188e-04, 2.2334e-04, 6.6494e-03,\n",
      "         1.0424e-02, 1.1559e-02, 1.4101e-04, 3.9059e-03, 1.1624e-03, 8.6919e-05,\n",
      "         9.8605e-04, 1.5249e-03, 1.9077e-03, 6.4279e-03, 4.3572e-04, 1.2978e-04,\n",
      "         1.1945e-04, 8.7399e-05, 1.0315e-03, 1.3668e-03, 1.4714e-03, 8.6490e-04,\n",
      "         2.7076e-04, 8.6649e-04, 1.5161e-04, 9.4044e-04, 3.4900e-04, 2.7401e-04,\n",
      "         1.6091e-04, 1.9479e-04, 6.4210e-04, 4.7936e-04, 1.9181e-04, 2.4663e-04,\n",
      "         1.9210e-04, 8.1985e-04, 1.5692e-04, 1.6518e-03, 7.0178e-03]])\n",
      "\n",
      "Sentence: The Israeli cybersecurity company linked the attack to a group known as APT35, which is also tracked using the codenames Charming Kitten, Phosphorus, and TA453, citing overlaps with toolsets previously identified as infrastructure used by the threat actor.\n",
      "Predicted Label: T1027\n",
      "Prediction Probabilities: tensor([[1.4372e-04, 4.4356e-04, 7.3851e-04, 4.4010e-03, 3.1310e-04, 5.9425e-03,\n",
      "         1.0754e-03, 1.6183e-03, 4.3202e-04, 1.4363e-04, 1.3854e-03, 2.3446e-02,\n",
      "         1.0234e-03, 5.2122e-05, 3.8263e-05, 8.6197e-04, 1.0123e-04, 1.5843e-04,\n",
      "         1.0343e-04, 1.0925e-04, 4.2921e-05, 2.0363e-03, 2.1428e-04, 2.4270e-04,\n",
      "         4.0942e-04, 7.0242e-04, 3.3469e-03, 3.0244e-04, 5.9168e-05, 2.3701e-04,\n",
      "         1.8647e-04, 1.0368e-02, 1.4636e-04, 1.9171e-04, 6.5574e-05, 1.6282e-04,\n",
      "         6.8010e-05, 1.0075e-03, 1.5904e-03, 6.0419e-04, 6.6956e-05, 7.2908e-04,\n",
      "         9.1570e-04, 2.7542e-04, 2.2221e-03, 3.8537e-03, 1.4588e-04, 3.7080e-03,\n",
      "         2.6218e-04, 1.2584e-04, 9.0475e-05, 1.5785e-04, 4.5988e-04, 4.0829e-03,\n",
      "         4.4307e-03, 1.2401e-04, 5.2072e-04, 2.1513e-04, 1.1258e-04, 4.1963e-05,\n",
      "         2.3752e-04, 8.2658e-04, 6.1836e-05, 2.6524e-02, 6.4928e-05, 6.0273e-05,\n",
      "         1.8919e-04, 6.7240e-05, 3.5232e-04, 1.6963e-03, 1.8942e-05, 4.1631e-04,\n",
      "         1.3319e-03, 3.1102e-04, 2.5556e-04, 8.4718e-04, 2.4544e-04, 1.9730e-03,\n",
      "         3.9869e-02, 1.3394e-04, 1.6560e-04, 1.5277e-04, 4.5124e-04, 4.4345e-05,\n",
      "         1.8834e-03, 8.2455e-04, 1.4633e-03, 2.0675e-04, 9.7677e-05, 4.5688e-04,\n",
      "         1.5388e-03, 7.2031e-05, 1.2995e-03, 4.4862e-04, 2.4519e-02, 5.7008e-05,\n",
      "         9.0623e-04, 1.9767e-04, 2.5299e-04, 5.9367e-05, 5.1950e-05, 6.2466e-04,\n",
      "         5.7639e-03, 6.5838e-04, 4.8943e-04, 1.7028e-02, 3.3025e-03, 3.3380e-03,\n",
      "         1.8488e-04, 1.0630e-04, 1.9449e-04, 2.8507e-02, 2.4690e-04, 5.5499e-05,\n",
      "         3.5696e-04, 1.5838e-04, 5.3515e-05, 1.0425e-03, 2.1127e-04, 1.8969e-04,\n",
      "         8.8053e-04, 2.9030e-03, 9.9229e-04, 9.4614e-05, 3.3695e-04, 1.1373e-04,\n",
      "         1.1079e-04, 6.9159e-04, 1.6436e-03, 1.1989e-02, 4.4805e-05, 5.0430e-05,\n",
      "         1.6691e-04, 6.8210e-05, 1.0819e-04, 1.0379e-04, 1.5974e-03, 5.3721e-04,\n",
      "         1.3605e-04, 6.8942e-05, 9.7985e-04, 7.8520e-04, 2.4079e-04, 6.6366e-03,\n",
      "         1.2645e-04, 1.8888e-04, 2.9050e-03, 2.0520e-04, 3.5823e-04, 1.8440e-04,\n",
      "         7.0508e-03, 1.5234e-03, 1.5342e-03, 1.2137e-04, 1.1915e-04, 8.4131e-05,\n",
      "         3.4994e-04, 3.0415e-04, 1.5990e-02, 1.2528e-03, 2.0273e-04, 9.7160e-05,\n",
      "         5.2395e-04, 2.1037e-04, 4.3171e-05, 2.9354e-04, 1.2723e-04, 1.0179e-04,\n",
      "         1.5279e-03, 1.1348e-04, 1.1332e-04, 2.1159e-04, 1.2564e-03, 3.7924e-04,\n",
      "         1.0237e-02, 1.2431e-04, 1.0257e-04, 2.6144e-03, 4.6501e-04, 9.4472e-05,\n",
      "         5.7566e-04, 5.3407e-04, 4.5260e-04, 1.1280e-04, 3.6075e-05, 6.4677e-05,\n",
      "         3.9329e-04, 1.9091e-04, 4.3605e-04, 1.6534e-04, 3.4707e-03, 9.3396e-04,\n",
      "         2.9781e-04, 4.6274e-03, 6.6978e-05, 3.8792e-05, 9.3484e-04, 8.1355e-05,\n",
      "         1.0633e-04, 1.5441e-04, 6.9000e-03, 1.1520e-04, 6.8494e-04, 1.6208e-03,\n",
      "         7.2513e-03, 4.1783e-04, 2.2329e-04, 1.1609e-04, 4.0501e-04, 5.4367e-05,\n",
      "         3.0475e-04, 7.9415e-05, 2.6851e-04, 3.8036e-04, 7.8165e-04, 4.1926e-05,\n",
      "         1.7095e-02, 4.4738e-05, 1.2655e-03, 1.8400e-03, 1.8413e-03, 8.2806e-04,\n",
      "         8.8215e-05, 1.5374e-04, 2.0596e-03, 7.9057e-02, 1.7285e-04, 2.0384e-03,\n",
      "         5.4655e-03, 1.5894e-03, 2.1628e-04, 2.4013e-04, 3.2103e-04, 1.8531e-03,\n",
      "         5.1392e-04, 6.8262e-04, 5.4866e-05, 4.8771e-04, 3.8586e-04, 1.2400e-04,\n",
      "         2.0240e-04, 1.3430e-04, 4.6953e-04, 2.6485e-02, 1.6281e-04, 3.5994e-04,\n",
      "         3.3304e-04, 3.9018e-03, 8.3170e-04, 3.3806e-04, 2.2481e-04, 2.2257e-03,\n",
      "         5.0021e-05, 3.4446e-05, 2.9126e-03, 5.8267e-03, 1.5773e-04, 3.8183e-04,\n",
      "         2.7929e-04, 3.8830e-04, 2.8300e-03, 2.1772e-04, 4.6059e-03, 7.4271e-04,\n",
      "         4.2188e-04, 1.1452e-04, 7.3495e-04, 1.0500e-04, 5.5518e-03, 8.1034e-05,\n",
      "         1.6784e-03, 5.1851e-04, 1.2122e-03, 1.3598e-04, 3.9582e-04, 4.2964e-04,\n",
      "         1.6582e-04, 3.5675e-04, 4.6172e-03, 1.2642e-04, 1.0194e-04, 4.0428e-04,\n",
      "         5.8899e-04, 1.0419e-03, 4.8628e-04, 7.4156e-05, 8.0025e-04, 1.0871e-02,\n",
      "         2.5698e-04, 1.5634e-04, 3.1031e-04, 4.9112e-04, 3.6316e-04, 1.3647e-03,\n",
      "         7.8177e-05, 5.4675e-05, 1.7834e-04, 2.8833e-04, 1.9165e-04, 1.0906e-04,\n",
      "         5.9262e-05, 2.1374e-04, 3.8919e-04, 3.1323e-04, 8.7542e-04, 5.1355e-04,\n",
      "         3.6465e-04, 2.4860e-04, 4.7529e-03, 3.4508e-04, 1.1839e-03, 1.3104e-04,\n",
      "         2.2208e-04, 1.6120e-04, 1.3000e-04, 7.0476e-04, 7.9528e-04, 2.9419e-04,\n",
      "         4.4415e-04, 1.3534e-04, 3.0176e-04, 8.3785e-03, 3.2111e-04, 2.3895e-02,\n",
      "         8.5136e-04, 4.7226e-05, 1.0852e-02, 8.0955e-04, 1.1964e-03, 3.0851e-04,\n",
      "         1.1379e-04, 8.2954e-04, 3.6262e-03, 2.5696e-03, 5.7762e-05, 2.4225e-04,\n",
      "         3.9372e-04, 4.3602e-03, 1.1686e-04, 7.3480e-04, 9.6523e-05, 8.0348e-05,\n",
      "         3.1289e-04, 1.3794e-04, 2.4953e-04, 5.7857e-05, 1.4170e-02, 6.4852e-05,\n",
      "         2.0820e-04, 1.1873e-04, 5.6348e-03, 7.9978e-03, 1.3180e-04, 4.5359e-04,\n",
      "         1.9761e-04, 7.7045e-05, 1.3234e-03, 4.8914e-05, 9.4492e-04, 1.0022e-04,\n",
      "         3.0691e-03, 2.4875e-04, 3.4160e-04, 2.8653e-04, 4.8601e-05, 6.3152e-05,\n",
      "         1.6817e-04, 3.4578e-04, 7.2657e-05, 9.0723e-05, 9.1694e-04, 7.6221e-05,\n",
      "         1.7850e-04, 4.1349e-04, 8.7156e-05, 1.3346e-04, 1.9386e-03, 1.9096e-03,\n",
      "         1.3435e-04, 2.1053e-04, 5.3665e-05, 7.6190e-05, 2.4505e-04, 2.6266e-03,\n",
      "         6.6603e-05, 7.8372e-05, 1.2281e-03, 1.2422e-04, 5.7975e-04, 4.4637e-05,\n",
      "         6.7151e-03, 1.3016e-04, 2.3139e-03, 1.4260e-03, 1.6715e-03, 6.7939e-03,\n",
      "         1.0922e-03, 1.1546e-04, 1.2813e-03, 2.9647e-04, 2.5038e-04, 1.2325e-02,\n",
      "         1.4955e-04, 1.5793e-04, 7.3965e-04, 1.5089e-04, 1.6505e-04, 2.8754e-04,\n",
      "         7.9838e-04, 8.7572e-04, 4.6137e-04, 2.2313e-03, 3.8371e-03, 1.3422e-04,\n",
      "         2.8975e-04, 6.2062e-05, 5.7340e-04, 2.3637e-03, 1.5376e-03, 7.6367e-05,\n",
      "         2.8084e-03, 4.1468e-04, 6.4421e-04, 8.8817e-04, 5.1009e-04, 2.5329e-03,\n",
      "         9.7767e-04, 3.1005e-04, 9.3544e-05, 1.2258e-04, 1.8362e-04, 1.8293e-04,\n",
      "         1.2809e-04, 1.1877e-04, 7.9645e-05, 9.4800e-04, 1.3669e-04, 5.3892e-04,\n",
      "         3.8681e-03, 1.2310e-02, 6.7895e-04, 1.5280e-04, 1.0699e-02, 9.8614e-05,\n",
      "         2.8306e-05, 3.5206e-04, 1.8513e-03, 2.0928e-04, 4.3048e-04, 2.6720e-03,\n",
      "         1.0548e-03, 1.5153e-04, 1.5742e-03, 8.0819e-04, 3.0924e-04, 1.1750e-03,\n",
      "         8.2297e-05, 1.6284e-03, 7.5736e-05, 1.3400e-03, 3.3302e-04, 2.6202e-04,\n",
      "         6.9798e-05, 8.8834e-05, 2.9298e-04, 1.2944e-01, 2.0457e-03, 9.9364e-04,\n",
      "         6.4421e-05, 3.5458e-04, 1.0243e-03, 1.0357e-04, 1.9147e-04, 7.2300e-03,\n",
      "         1.0856e-02, 4.8214e-03, 1.1064e-04, 1.2983e-02, 6.7321e-04, 5.4926e-05,\n",
      "         3.9181e-04, 1.6858e-03, 2.1677e-03, 2.8408e-03, 2.2840e-04, 1.0000e-04,\n",
      "         8.6401e-05, 6.1079e-05, 4.9870e-04, 1.0195e-03, 1.3739e-03, 8.0386e-04,\n",
      "         1.6327e-04, 3.3017e-04, 1.0185e-04, 8.4643e-04, 3.1540e-04, 1.9598e-04,\n",
      "         1.1605e-04, 1.2523e-04, 7.0379e-04, 5.0289e-04, 1.2249e-04, 1.3636e-04,\n",
      "         3.0487e-04, 1.3062e-03, 9.9767e-05, 2.7663e-03, 2.2815e-03]])\n",
      "\n",
      "Sentence: Cybersecurity Log4Shell aka CVE-2021-44228 (CVSS score: 10.0) concerns a critical security vulnerability in the popular Log4j logging library that, if successfully exploited, could lead to remote execution of arbitrary code on compromised systems.\n",
      "Predicted Label: T1203\n",
      "Prediction Probabilities: tensor([[4.5600e-04, 6.7992e-04, 1.9801e-03, 7.5741e-04, 7.6307e-04, 4.5670e-03,\n",
      "         9.0754e-04, 1.4284e-03, 1.0118e-03, 5.4780e-04, 3.3420e-03, 1.4380e-03,\n",
      "         7.3174e-04, 3.3525e-04, 4.5662e-04, 7.7684e-04, 4.4364e-04, 7.4912e-04,\n",
      "         3.1451e-04, 4.4601e-04, 5.9458e-04, 2.0890e-03, 5.4464e-04, 1.2018e-03,\n",
      "         7.5193e-04, 9.3502e-04, 1.6910e-03, 7.5252e-04, 1.8902e-04, 5.3372e-04,\n",
      "         1.1072e-03, 5.6905e-03, 3.6558e-04, 4.3931e-04, 3.0304e-04, 5.9993e-04,\n",
      "         4.7184e-04, 9.4343e-04, 5.3600e-04, 8.4814e-04, 2.5613e-04, 7.1118e-04,\n",
      "         1.2960e-03, 4.2948e-04, 1.6427e-03, 1.6608e-03, 5.6388e-04, 2.7495e-03,\n",
      "         3.3037e-04, 6.0358e-04, 3.9711e-04, 6.2387e-04, 6.9441e-04, 1.0523e-03,\n",
      "         1.4164e-03, 4.2093e-04, 5.2207e-04, 8.6008e-04, 3.7926e-04, 3.1307e-04,\n",
      "         9.7601e-04, 8.4104e-04, 4.4812e-04, 1.7321e-03, 5.3820e-04, 3.0384e-04,\n",
      "         8.8385e-04, 2.7998e-04, 4.6301e-04, 6.1537e-04, 2.5587e-04, 3.6930e-04,\n",
      "         1.0718e-03, 8.0262e-04, 6.6943e-04, 8.8383e-04, 8.1193e-04, 8.8138e-04,\n",
      "         2.3366e-03, 4.7925e-04, 2.9494e-04, 5.8683e-04, 5.6516e-04, 4.8769e-04,\n",
      "         1.8217e-03, 1.0151e-03, 4.6941e-04, 4.3487e-04, 4.3137e-04, 7.5743e-04,\n",
      "         6.9401e-04, 3.2127e-04, 1.1274e-03, 8.0409e-04, 1.7024e-03, 3.6751e-04,\n",
      "         8.0361e-04, 3.6071e-04, 3.9261e-04, 5.1495e-04, 4.0437e-04, 6.9608e-04,\n",
      "         1.0560e-03, 5.6028e-04, 6.6449e-04, 3.1736e-03, 7.8887e-04, 3.6597e-03,\n",
      "         3.6283e-04, 9.2597e-04, 5.5749e-04, 9.9024e-04, 7.5356e-04, 3.6402e-04,\n",
      "         1.0474e-03, 5.8179e-04, 2.8584e-04, 6.1206e-04, 5.5277e-04, 5.5331e-04,\n",
      "         5.5247e-04, 1.0779e-03, 7.2950e-04, 3.9457e-04, 6.6808e-04, 4.9058e-04,\n",
      "         5.9825e-04, 5.5093e-04, 2.3118e-03, 3.5102e-03, 4.5433e-04, 3.2073e-04,\n",
      "         7.6251e-04, 2.5993e-04, 7.3600e-04, 3.5293e-04, 1.0326e-03, 8.0728e-04,\n",
      "         6.0012e-04, 2.1906e-04, 1.2827e-03, 6.0620e-04, 5.2124e-04, 1.3979e-03,\n",
      "         4.6134e-04, 5.0892e-04, 1.3255e-03, 6.3398e-04, 6.5821e-04, 5.4722e-04,\n",
      "         1.8506e-03, 4.7928e-04, 1.5505e-03, 4.6551e-04, 7.0712e-04, 5.2255e-04,\n",
      "         9.3181e-04, 6.1150e-04, 5.2213e-03, 6.4768e-04, 4.6056e-04, 5.1547e-04,\n",
      "         7.2944e-04, 5.5494e-04, 4.0592e-04, 9.5927e-04, 4.4812e-04, 2.6300e-04,\n",
      "         1.0923e-03, 2.3597e-04, 4.5610e-04, 5.7870e-04, 6.8706e-04, 5.5910e-04,\n",
      "         1.5139e-03, 6.3344e-04, 4.7696e-04, 1.2001e-03, 9.5239e-04, 3.8007e-04,\n",
      "         1.2267e-03, 9.5414e-04, 1.0388e-03, 7.1808e-04, 2.8289e-04, 3.3375e-04,\n",
      "         4.1206e-04, 4.9594e-04, 6.3492e-04, 5.5647e-04, 1.3985e-03, 7.4713e-04,\n",
      "         7.2655e-04, 5.4290e-04, 5.4952e-04, 4.2248e-04, 1.0306e-03, 2.5001e-04,\n",
      "         3.9724e-04, 4.4966e-04, 2.3418e-03, 2.4533e-04, 5.8438e-04, 1.0243e-03,\n",
      "         8.6903e-04, 1.0551e-03, 6.8380e-04, 7.5926e-04, 8.4898e-04, 3.2159e-04,\n",
      "         6.8557e-04, 3.8824e-04, 1.1689e-03, 6.2848e-04, 7.0974e-04, 2.0794e-04,\n",
      "         1.9032e-03, 2.3856e-04, 2.0199e-03, 1.3488e-03, 1.8249e-03, 7.3358e-04,\n",
      "         7.9210e-04, 3.7589e-04, 1.5412e-03, 2.5295e-03, 4.8593e-04, 1.2170e-03,\n",
      "         9.4463e-04, 1.3927e-03, 8.8644e-04, 8.8485e-04, 6.6603e-04, 1.2545e-03,\n",
      "         6.3288e-04, 5.3930e-04, 3.2248e-04, 6.3975e-04, 4.8404e-04, 5.2212e-04,\n",
      "         5.5345e-04, 5.5271e-04, 4.6598e-04, 1.4850e-03, 5.3668e-04, 5.4053e-04,\n",
      "         8.1667e-04, 1.6321e-03, 7.8883e-04, 7.1390e-04, 5.9402e-04, 3.9698e-04,\n",
      "         3.2260e-04, 2.9155e-04, 8.9632e-04, 1.5889e-03, 7.2668e-04, 3.9246e-04,\n",
      "         8.6976e-04, 7.4770e-04, 8.7968e-04, 1.0854e-03, 1.9645e-03, 1.6043e-03,\n",
      "         9.1299e-04, 3.3195e-04, 5.9556e-04, 8.5835e-04, 2.1272e-03, 2.7485e-04,\n",
      "         1.1719e-03, 7.7247e-04, 7.4133e-04, 6.5370e-04, 8.0324e-04, 9.1261e-04,\n",
      "         6.4458e-04, 5.8639e-04, 1.3489e-03, 2.6579e-04, 6.4757e-04, 8.2353e-04,\n",
      "         3.2386e-03, 1.0525e-03, 8.7326e-04, 2.8789e-04, 1.5598e-02, 1.4424e-03,\n",
      "         5.7038e-04, 8.9106e-04, 3.6408e-04, 8.1051e-04, 8.1376e-04, 7.3423e-04,\n",
      "         3.1775e-04, 3.3046e-04, 8.0401e-04, 7.4142e-04, 3.1188e-04, 5.2250e-04,\n",
      "         2.7641e-04, 3.4527e-04, 7.3406e-04, 7.2426e-04, 1.0542e-03, 3.6488e-04,\n",
      "         6.3446e-04, 2.9984e-04, 1.2424e-03, 7.9953e-04, 1.2141e-03, 4.8764e-04,\n",
      "         3.2112e-04, 4.6175e-04, 4.0555e-04, 7.9027e-04, 9.2887e-04, 3.9309e-04,\n",
      "         8.3460e-04, 4.7710e-04, 6.1925e-04, 2.0216e-03, 6.7131e-04, 1.1425e-03,\n",
      "         9.7507e-04, 5.1639e-04, 1.4564e-03, 1.2339e-03, 7.3122e-04, 5.5902e-04,\n",
      "         7.6684e-04, 5.4644e-04, 2.3512e-03, 1.1297e-03, 4.7503e-04, 7.7479e-04,\n",
      "         9.6022e-04, 5.3272e-04, 4.5413e-04, 1.8538e-03, 4.6608e-04, 6.4484e-04,\n",
      "         4.3774e-04, 9.4714e-04, 3.1436e-04, 2.2904e-04, 8.5761e-04, 6.0714e-04,\n",
      "         4.8428e-04, 3.9219e-04, 3.5581e-03, 9.3728e-04, 4.6888e-04, 7.8662e-04,\n",
      "         5.0204e-04, 5.1798e-04, 9.1235e-04, 2.0330e-04, 9.4147e-04, 4.5652e-04,\n",
      "         9.0049e-02, 8.0088e-04, 6.7739e-04, 4.7075e-04, 3.2731e-04, 3.8871e-04,\n",
      "         6.1745e-04, 5.5000e-04, 4.5019e-04, 4.2651e-04, 7.9692e-04, 5.2311e-04,\n",
      "         5.3580e-04, 5.5418e-04, 3.4234e-04, 6.5693e-04, 1.2630e-03, 2.0586e-03,\n",
      "         4.1264e-04, 4.0922e-04, 1.9689e-04, 3.5539e-04, 5.4953e-04, 1.4702e-03,\n",
      "         3.1893e-04, 2.7656e-04, 1.9630e-03, 3.2288e-04, 1.0439e-03, 5.1528e-04,\n",
      "         1.0471e-03, 7.8710e-04, 7.0769e-04, 1.2416e-03, 1.4021e-03, 4.3944e-04,\n",
      "         4.8238e-04, 5.3546e-04, 1.7514e-03, 6.9220e-04, 5.7619e-04, 2.5164e-03,\n",
      "         4.1352e-04, 9.8938e-04, 7.5005e-03, 6.3655e-04, 7.5132e-04, 5.8648e-04,\n",
      "         1.0506e-03, 1.6102e-03, 7.4815e-04, 1.2933e-03, 5.5782e-04, 5.5098e-04,\n",
      "         7.6804e-04, 3.0061e-04, 1.2757e-03, 1.4937e-03, 1.4384e-03, 2.5739e-04,\n",
      "         1.0273e-03, 8.2537e-04, 9.7386e-04, 5.2120e-04, 8.3741e-04, 1.1004e-03,\n",
      "         8.0003e-04, 9.9957e-04, 7.9757e-04, 6.3633e-04, 4.0848e-04, 3.4063e-04,\n",
      "         6.7190e-04, 1.9106e-04, 5.0885e-04, 9.4411e-04, 9.5160e-04, 4.4145e-04,\n",
      "         1.7848e-03, 1.1678e-03, 1.2021e-03, 5.3323e-04, 1.2029e-03, 3.8136e-04,\n",
      "         3.8174e-04, 5.7177e-04, 7.5526e-04, 3.6825e-04, 1.0348e-03, 1.5653e-03,\n",
      "         5.1265e-04, 3.8845e-04, 1.9705e-01, 1.6658e-03, 6.6503e-04, 7.4341e-04,\n",
      "         3.0907e-04, 1.3393e-03, 3.1429e-04, 1.9380e-03, 4.5022e-04, 7.8376e-04,\n",
      "         3.6827e-04, 4.8380e-04, 6.5638e-04, 2.5374e-03, 1.4433e-03, 5.8565e-04,\n",
      "         3.3102e-04, 4.7448e-04, 6.0832e-04, 5.1070e-04, 5.9894e-04, 5.4540e-03,\n",
      "         1.4121e-03, 1.4566e-03, 4.5239e-04, 2.6291e-01, 9.0288e-04, 3.0883e-04,\n",
      "         8.2391e-04, 1.3624e-03, 1.4115e-03, 3.8644e-03, 4.5568e-04, 4.0377e-04,\n",
      "         4.1156e-04, 3.0286e-04, 7.5990e-04, 9.5664e-04, 1.1556e-03, 1.0409e-03,\n",
      "         6.4110e-04, 1.2899e-03, 5.6614e-04, 3.7953e-04, 4.8285e-04, 4.9266e-04,\n",
      "         5.9543e-04, 3.3745e-04, 4.8402e-04, 6.4078e-04, 5.5419e-04, 4.7967e-04,\n",
      "         6.1747e-04, 1.9404e-03, 3.1912e-04, 1.5480e-03, 3.6654e-04]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./final_model1')\n",
    "# model = BertForSequenceClassification.from_pretrained('ibm-research/CTI-BERT')\n",
    "tokenizer = BertTokenizer.from_pretrained('ibm-research/CTI-BERT')\n",
    "\n",
    "# # Assuming you have a list of labels\n",
    "# labels = ['Label_0', 'Label_1', 'Label_2']  # Update with your actual labels\n",
    "\n",
    "# Function to get prediction for manual input text\n",
    "def predict(input_text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Get the model's output\n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the logits (model output before applying softmax)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Apply softmax to get probabilities (for multi-class classification)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get the predicted label (index of max probability)\n",
    "    predicted_label = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    return predicted_label, probabilities\n",
    "\n",
    "# Function to break text into sentences using nltk\n",
    "def sentence_tokenizer(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Test with article input\n",
    "input_text = \"\"\"\n",
    "An Iranian state-sponsored actor has been observed scanning and attempting to abuse the Log4Shell flaw in publicly-exposed Java applications to deploy a hitherto undocumented PowerShell-based modular backdoor dubbed \"CharmPower\" for follow-on post-exploitation. \"The actor's attack setup was obviously rushed, as they used the basic open-source tool for the exploitation and based their operations on previous infrastructure, which made the attack easier to detect and attribute,\" researchers from Check Point said in a report published this week. The Israeli cybersecurity company linked the attack to a group known as APT35, which is also tracked using the codenames Charming Kitten, Phosphorus, and TA453, citing overlaps with toolsets previously identified as infrastructure used by the threat actor. Cybersecurity Log4Shell aka CVE-2021-44228 (CVSS score: 10.0) concerns a critical security vulnerability in the popular Log4j logging library that, if successfully exploited, could lead to remote execution of arbitrary code on compromised systems.\n",
    "\"\"\"\n",
    "\n",
    "# Break the article into sentences\n",
    "sentences = sentence_tokenizer(input_text)\n",
    "\n",
    "# Process each sentence through the model\n",
    "for sentence in sentences:\n",
    "    predicted_label, probabilities = predict(sentence)\n",
    "    \n",
    "    # Map the predicted label index to the label name\n",
    "    predicted_label_name = labels[predicted_label]\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Predicted Label: {predicted_label_name}\")\n",
    "    print(f\"Prediction Probabilities: {probabilities}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16fe92b1-8e87-4bdb-b731-d81aae939b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: An Iranian state-sponsored actor has been observed scanning and attempting to abuse the Log4Shell flaw in publicly-exposed Java applications to deploy a hitherto undocumented PowerShell-based modular backdoor dubbed \"CharmPower\" for follow-on post-exploitation.\n",
      "Predicted Label: T1190\n",
      "Prediction Probabilities: tensor([[5.1243e-04, 8.1736e-04, 1.9886e-03, 6.2065e-04, 5.7447e-04, 1.0098e-02,\n",
      "         8.5801e-04, 1.2725e-03, 1.2196e-03, 7.8081e-04, 1.6700e-03, 1.7396e-03,\n",
      "         9.5309e-04, 6.1896e-04, 5.0379e-04, 9.5596e-04, 5.8830e-04, 8.1963e-04,\n",
      "         4.7799e-04, 8.1372e-04, 7.2214e-04, 1.5212e-03, 5.9348e-04, 1.8004e-03,\n",
      "         8.8915e-04, 9.3078e-04, 1.5904e-03, 1.3662e-03, 2.5677e-04, 6.9799e-04,\n",
      "         2.1121e-03, 1.4003e-03, 5.5481e-04, 9.2005e-04, 3.9183e-04, 9.1157e-04,\n",
      "         7.2661e-04, 6.3973e-04, 5.8083e-04, 1.6261e-03, 3.4794e-04, 7.4668e-04,\n",
      "         5.6557e-04, 5.8662e-04, 1.4305e-03, 1.1343e-03, 7.2599e-04, 3.5753e-03,\n",
      "         3.4277e-04, 1.0854e-03, 6.3097e-04, 8.2120e-04, 9.9269e-04, 7.8947e-04,\n",
      "         2.8406e-03, 4.7506e-04, 6.0557e-04, 1.6740e-03, 7.7277e-04, 5.1364e-04,\n",
      "         1.0923e-03, 8.1990e-04, 4.9303e-04, 2.3848e-03, 8.8856e-04, 5.2135e-04,\n",
      "         1.0646e-03, 4.1800e-04, 6.7472e-04, 3.7980e-04, 3.4286e-04, 5.0164e-04,\n",
      "         9.0453e-04, 1.0751e-03, 9.7602e-04, 1.4151e-03, 1.5825e-03, 7.5775e-04,\n",
      "         3.5918e-03, 6.6382e-04, 6.0247e-04, 7.2050e-04, 7.8343e-04, 7.3652e-04,\n",
      "         2.1696e-03, 9.4258e-04, 4.7391e-04, 4.2502e-04, 4.5233e-04, 5.6211e-04,\n",
      "         1.2448e-03, 4.5156e-04, 1.1048e-03, 9.5195e-04, 2.3674e-03, 3.9987e-04,\n",
      "         7.3781e-04, 7.9430e-04, 4.6063e-04, 7.9745e-04, 4.3674e-04, 9.8183e-04,\n",
      "         1.0036e-03, 5.1707e-04, 1.1207e-03, 1.8642e-03, 1.5785e-03, 2.2707e-03,\n",
      "         6.2485e-04, 9.1143e-04, 5.0937e-04, 3.2911e-04, 1.3800e-03, 5.0894e-04,\n",
      "         1.7210e-03, 9.6190e-04, 3.0432e-04, 7.4522e-04, 7.5083e-04, 8.4669e-04,\n",
      "         6.2444e-04, 7.9125e-04, 7.7421e-04, 6.0314e-04, 6.3758e-04, 7.8981e-04,\n",
      "         5.0465e-04, 8.1845e-04, 1.3001e-03, 3.5372e-03, 6.4596e-04, 4.4983e-04,\n",
      "         1.4211e-03, 5.3132e-04, 1.1013e-03, 5.3540e-04, 1.7902e-03, 1.3951e-03,\n",
      "         9.1982e-04, 2.4836e-04, 2.1906e-03, 9.9300e-04, 6.0863e-04, 1.6525e-03,\n",
      "         6.0721e-04, 5.5797e-04, 1.5870e-03, 7.0096e-04, 8.4636e-04, 9.5003e-04,\n",
      "         2.6727e-03, 8.1710e-04, 1.4676e-03, 6.6669e-04, 7.3291e-04, 6.3375e-04,\n",
      "         1.3052e-03, 9.4065e-04, 1.7072e-03, 1.2044e-03, 5.4950e-04, 6.0376e-04,\n",
      "         1.0182e-03, 6.5832e-04, 6.0660e-04, 1.3168e-03, 7.4784e-04, 5.1947e-04,\n",
      "         6.2563e-04, 3.5392e-04, 5.5044e-04, 9.0647e-04, 1.5003e-03, 1.0118e-03,\n",
      "         1.0640e-03, 7.7854e-04, 5.3642e-04, 1.7772e-03, 1.0182e-03, 3.9095e-04,\n",
      "         1.6100e-03, 1.2654e-03, 1.3693e-03, 9.3447e-04, 3.3893e-04, 3.6751e-04,\n",
      "         4.5917e-04, 5.3248e-04, 1.2397e-03, 6.7460e-04, 1.2231e-03, 8.8187e-04,\n",
      "         7.7462e-04, 6.0208e-04, 7.4840e-04, 5.4427e-04, 8.3332e-04, 4.3197e-04,\n",
      "         4.3367e-04, 5.6313e-04, 2.3886e-03, 3.6888e-04, 9.7839e-04, 1.0184e-03,\n",
      "         4.7828e-04, 1.1816e-03, 1.1277e-03, 1.5930e-03, 9.5645e-04, 4.3940e-04,\n",
      "         9.8370e-04, 5.7964e-04, 1.0838e-03, 7.6799e-04, 2.4778e-03, 3.4629e-04,\n",
      "         8.1206e-04, 2.4215e-04, 1.4674e-03, 3.1474e-03, 2.4425e-03, 1.7653e-03,\n",
      "         1.1160e-03, 5.6634e-04, 1.5964e-03, 2.2174e-03, 7.4174e-04, 9.3532e-04,\n",
      "         7.3771e-04, 1.1701e-03, 1.0820e-03, 1.0261e-03, 9.1649e-04, 8.7688e-04,\n",
      "         6.9612e-04, 6.8039e-04, 5.4766e-04, 7.1697e-04, 5.4071e-04, 9.9685e-04,\n",
      "         6.2672e-04, 7.1613e-04, 5.4243e-04, 1.9587e-03, 8.0976e-04, 6.3086e-04,\n",
      "         9.8227e-04, 2.6479e-03, 9.8493e-04, 1.0197e-03, 8.1711e-04, 3.8516e-04,\n",
      "         4.9008e-04, 3.9403e-04, 1.3583e-03, 1.0985e-03, 1.0062e-03, 7.5188e-04,\n",
      "         1.3995e-03, 9.8327e-04, 6.9101e-04, 1.3998e-03, 1.5199e-03, 1.1598e-03,\n",
      "         1.1385e-03, 4.9174e-04, 8.2618e-04, 1.1082e-03, 2.3803e-03, 4.9122e-04,\n",
      "         1.3056e-03, 8.0618e-04, 8.0578e-04, 5.5699e-04, 1.5431e-03, 1.1172e-03,\n",
      "         8.8841e-04, 8.0430e-04, 1.4020e-03, 3.4177e-04, 1.1613e-03, 1.1185e-03,\n",
      "         5.2509e-03, 1.2700e-03, 1.5479e-03, 4.6240e-04, 2.7439e-02, 6.4221e-04,\n",
      "         5.6731e-04, 1.6803e-03, 4.0586e-04, 1.4167e-03, 9.6965e-04, 7.6690e-04,\n",
      "         5.3787e-04, 4.7208e-04, 1.1300e-03, 1.3031e-03, 4.2672e-04, 7.5806e-04,\n",
      "         4.8877e-04, 5.7150e-04, 1.0753e-03, 1.0643e-03, 1.4725e-03, 5.9935e-04,\n",
      "         1.1324e-03, 5.4330e-04, 9.3166e-04, 8.8199e-04, 1.2781e-03, 7.9710e-04,\n",
      "         3.4545e-04, 5.8507e-04, 6.9614e-04, 8.3752e-04, 1.5376e-03, 6.0415e-04,\n",
      "         1.0500e-03, 8.2090e-04, 8.9977e-04, 2.6797e-03, 5.9224e-04, 3.0639e-03,\n",
      "         1.0547e-03, 6.7370e-04, 1.0782e-03, 1.7838e-03, 9.3115e-04, 9.6521e-04,\n",
      "         1.2197e-03, 5.7681e-04, 1.8918e-03, 1.0899e-03, 6.6093e-04, 7.9161e-04,\n",
      "         8.5509e-04, 3.2305e-04, 5.2389e-04, 2.8866e-03, 8.1623e-04, 9.8354e-04,\n",
      "         1.0119e-03, 8.3424e-04, 3.4408e-04, 3.4070e-04, 7.2469e-04, 5.4851e-04,\n",
      "         7.6138e-04, 5.6089e-04, 2.0153e-03, 8.9098e-04, 6.3995e-04, 1.1530e-03,\n",
      "         7.2014e-04, 9.5478e-04, 6.3742e-04, 4.4482e-04, 1.1085e-03, 7.0640e-04,\n",
      "         6.5187e-02, 8.2299e-04, 7.7831e-04, 5.8598e-04, 4.8181e-04, 6.9388e-04,\n",
      "         8.3626e-04, 7.8130e-04, 5.8227e-04, 6.5210e-04, 1.5648e-03, 8.7877e-04,\n",
      "         6.4324e-04, 8.7134e-04, 5.3562e-04, 1.1821e-03, 1.2837e-03, 5.0449e-03,\n",
      "         6.8156e-04, 4.4907e-04, 2.5478e-04, 4.6732e-04, 6.3125e-04, 1.3079e-03,\n",
      "         6.2251e-04, 3.4519e-04, 4.5126e-03, 4.4516e-04, 1.8245e-03, 1.2641e-03,\n",
      "         1.6772e-03, 7.5292e-04, 6.7760e-04, 1.5597e-03, 1.4807e-03, 6.0726e-04,\n",
      "         4.9301e-04, 6.6333e-04, 1.4092e-03, 8.0118e-04, 1.4755e-03, 2.1295e-03,\n",
      "         5.2061e-04, 1.4736e-03, 9.7616e-03, 1.1445e-03, 1.2963e-03, 7.2706e-04,\n",
      "         1.2381e-03, 1.1816e-03, 1.5545e-03, 1.2844e-03, 1.2931e-03, 7.9280e-04,\n",
      "         7.8399e-04, 4.3629e-04, 1.9195e-03, 1.3654e-03, 1.3378e-03, 3.0695e-04,\n",
      "         8.6873e-04, 1.2769e-03, 1.1099e-03, 3.6963e-04, 1.7085e-03, 8.8898e-04,\n",
      "         6.2310e-04, 9.8690e-04, 1.3942e-03, 8.7356e-04, 6.0690e-04, 4.1897e-04,\n",
      "         9.2194e-04, 3.3385e-04, 5.9560e-04, 9.5969e-04, 1.3536e-03, 4.6891e-04,\n",
      "         1.5969e-03, 1.4391e-03, 1.4306e-03, 9.7905e-04, 1.3592e-03, 5.8313e-04,\n",
      "         4.6881e-04, 5.8981e-04, 1.4116e-03, 6.6957e-04, 1.3278e-03, 1.6781e-03,\n",
      "         1.0665e-03, 5.3685e-04, 3.3573e-01, 1.5986e-03, 6.2589e-04, 6.1272e-04,\n",
      "         4.2267e-04, 1.0364e-03, 5.2023e-04, 1.5465e-03, 5.6832e-04, 8.1431e-04,\n",
      "         5.1207e-04, 6.0911e-04, 1.0685e-03, 8.9923e-04, 1.3330e-03, 6.8612e-04,\n",
      "         5.0570e-04, 6.7420e-04, 9.4479e-04, 8.7794e-04, 7.7393e-04, 3.4351e-03,\n",
      "         1.3035e-03, 5.5148e-03, 7.8224e-04, 3.3190e-02, 1.3089e-03, 3.7899e-04,\n",
      "         1.6999e-03, 1.1523e-03, 8.2234e-04, 1.5998e-02, 5.6206e-04, 5.1467e-04,\n",
      "         5.0084e-04, 5.1946e-04, 1.1729e-03, 1.2059e-03, 1.0861e-03, 1.1384e-03,\n",
      "         7.4544e-04, 1.9657e-03, 8.0707e-04, 5.6829e-04, 5.0728e-04, 6.8754e-04,\n",
      "         7.9362e-04, 5.7924e-04, 5.4745e-04, 8.7257e-04, 7.5891e-04, 7.4350e-04,\n",
      "         8.4959e-04, 1.1139e-03, 4.9686e-04, 1.1441e-03, 8.6004e-04]])\n",
      "\n",
      "Sentence: \"The actor's attack setup was obviously rushed, as they used the basic open-source tool for the exploitation and based their operations on previous infrastructure, which made the attack easier to detect and attribute,\" researchers from Check Point said in a report published this week.\n",
      "Predicted Label: T1588.002\n",
      "Prediction Probabilities: tensor([[2.4026e-04, 5.1910e-04, 1.0565e-03, 3.9828e-03, 3.1243e-04, 9.5403e-03,\n",
      "         1.4583e-03, 2.1606e-03, 6.1456e-04, 2.3332e-04, 1.2460e-03, 8.7432e-03,\n",
      "         1.7473e-03, 1.3241e-04, 6.5530e-05, 1.5928e-03, 1.9085e-04, 1.5920e-04,\n",
      "         1.3796e-04, 1.6315e-04, 1.0679e-04, 3.3029e-03, 2.0347e-04, 5.4766e-04,\n",
      "         4.0755e-04, 9.6830e-04, 2.0340e-03, 6.6337e-04, 6.8181e-05, 2.3092e-04,\n",
      "         3.6159e-04, 4.2326e-03, 2.0716e-04, 5.8377e-04, 8.3595e-05, 1.7467e-04,\n",
      "         1.2474e-04, 9.7990e-04, 2.7379e-03, 1.0200e-03, 8.9916e-05, 6.6498e-04,\n",
      "         5.2855e-04, 3.2648e-04, 1.2466e-03, 3.1268e-03, 1.7028e-04, 1.9809e-03,\n",
      "         3.1151e-04, 2.2085e-04, 1.7249e-04, 2.5977e-04, 7.6367e-04, 3.1097e-03,\n",
      "         1.0078e-01, 1.3100e-04, 7.1032e-04, 6.5931e-04, 2.7081e-04, 8.5142e-05,\n",
      "         3.9628e-04, 6.4399e-04, 7.0504e-05, 1.7035e-02, 1.4661e-04, 8.8866e-05,\n",
      "         3.0256e-04, 9.1881e-05, 4.7209e-04, 1.1429e-03, 3.6253e-05, 8.7079e-04,\n",
      "         1.1748e-03, 5.0208e-04, 3.2326e-04, 1.0244e-03, 4.2132e-04, 2.8532e-03,\n",
      "         3.6741e-02, 1.6451e-04, 2.6619e-04, 2.7718e-04, 9.3802e-04, 1.1084e-04,\n",
      "         1.2758e-03, 7.6585e-04, 2.3437e-03, 3.0271e-04, 1.0860e-04, 3.0686e-04,\n",
      "         5.6236e-03, 9.4120e-05, 2.1701e-03, 4.1997e-04, 2.6162e-02, 8.0173e-05,\n",
      "         1.2530e-03, 4.7745e-04, 3.8465e-04, 1.3472e-04, 7.8339e-05, 5.9386e-04,\n",
      "         8.7709e-03, 6.7637e-04, 5.9147e-04, 4.0978e-03, 4.3848e-03, 3.1085e-03,\n",
      "         5.1444e-04, 1.4336e-04, 2.6197e-04, 9.4310e-03, 6.2556e-04, 8.8702e-05,\n",
      "         5.8306e-04, 3.1878e-04, 9.2933e-05, 9.4693e-04, 1.8189e-04, 2.5677e-04,\n",
      "         1.1603e-03, 1.5813e-03, 1.0241e-03, 1.2494e-04, 3.5095e-04, 1.5088e-04,\n",
      "         1.7983e-04, 7.1352e-04, 1.5817e-03, 2.3700e-02, 7.7513e-05, 4.7811e-05,\n",
      "         5.0001e-04, 1.4514e-04, 2.1813e-04, 1.4517e-04, 2.0661e-03, 1.5516e-03,\n",
      "         1.8178e-04, 6.4570e-05, 1.7783e-03, 1.8435e-03, 2.6321e-04, 1.2203e-02,\n",
      "         1.9562e-04, 2.9861e-04, 1.0885e-02, 3.2522e-04, 4.0642e-04, 3.2208e-04,\n",
      "         5.5042e-03, 1.2759e-03, 2.3183e-03, 1.3984e-04, 1.3893e-04, 1.2893e-04,\n",
      "         6.8938e-04, 5.8264e-04, 7.3926e-03, 3.1725e-03, 2.8457e-04, 1.2948e-04,\n",
      "         1.2270e-03, 3.4374e-04, 9.4419e-05, 4.3198e-04, 1.7541e-04, 2.2665e-04,\n",
      "         1.2255e-03, 1.1611e-04, 1.7912e-04, 2.6852e-04, 5.2183e-03, 1.0248e-03,\n",
      "         3.0007e-03, 1.7742e-04, 2.0496e-04, 3.4823e-03, 5.1666e-04, 1.1372e-04,\n",
      "         1.0149e-03, 4.5606e-04, 5.4207e-04, 1.4427e-04, 6.5734e-05, 1.0935e-04,\n",
      "         4.6912e-04, 2.0663e-04, 1.2845e-03, 2.7281e-04, 1.0719e-03, 1.9857e-03,\n",
      "         3.6559e-04, 9.0161e-03, 1.2390e-04, 6.4467e-05, 8.0203e-04, 1.3944e-04,\n",
      "         1.3935e-04, 1.9699e-04, 7.9364e-03, 1.5402e-04, 8.9187e-04, 2.9042e-03,\n",
      "         3.8214e-03, 5.9849e-04, 4.1867e-04, 3.0117e-04, 7.9174e-04, 8.4080e-05,\n",
      "         4.5575e-04, 2.5009e-04, 2.7350e-04, 5.6306e-04, 2.6565e-03, 6.9600e-05,\n",
      "         2.6386e-03, 5.1799e-05, 1.5346e-03, 3.4519e-03, 4.1343e-03, 1.3224e-03,\n",
      "         1.7942e-04, 2.5841e-04, 2.0664e-03, 2.6728e-02, 2.0794e-04, 1.2038e-03,\n",
      "         4.2139e-03, 1.5646e-03, 1.9407e-04, 3.1608e-04, 4.0538e-04, 1.4134e-03,\n",
      "         5.8435e-04, 1.0074e-03, 1.2200e-04, 7.6251e-04, 5.8315e-04, 2.3613e-04,\n",
      "         1.9529e-04, 1.3202e-04, 7.0461e-04, 1.4001e-02, 2.0203e-04, 3.9996e-04,\n",
      "         5.4822e-04, 1.0885e-02, 7.4280e-04, 5.1051e-04, 3.6773e-04, 1.9855e-03,\n",
      "         5.7470e-05, 5.9309e-05, 1.5101e-03, 2.3074e-03, 2.4175e-04, 8.7756e-04,\n",
      "         4.2923e-04, 5.1171e-04, 2.2404e-03, 4.1949e-04, 4.2347e-03, 7.8590e-04,\n",
      "         4.4712e-04, 2.1370e-04, 1.1643e-03, 1.5111e-04, 1.5197e-02, 2.0587e-04,\n",
      "         2.4382e-03, 3.4363e-04, 1.1481e-03, 2.0883e-04, 6.8906e-04, 5.6662e-04,\n",
      "         2.9727e-04, 5.3280e-04, 4.1264e-03, 1.6398e-04, 1.3065e-04, 6.4269e-04,\n",
      "         8.3028e-04, 1.4273e-03, 9.9006e-04, 1.1118e-04, 1.5129e-03, 3.7069e-03,\n",
      "         3.3779e-04, 2.8666e-04, 4.7855e-04, 9.6999e-04, 4.9411e-04, 1.6530e-03,\n",
      "         1.5214e-04, 1.2687e-04, 2.2756e-04, 4.1257e-04, 2.7393e-04, 1.8741e-04,\n",
      "         9.4537e-05, 3.3823e-04, 4.6645e-04, 2.8541e-04, 4.4892e-04, 5.6196e-04,\n",
      "         4.3319e-04, 6.8703e-04, 6.4178e-03, 9.1687e-04, 1.5492e-03, 2.1402e-04,\n",
      "         2.6273e-04, 2.3794e-04, 1.5324e-04, 9.7846e-04, 8.5019e-04, 4.5168e-04,\n",
      "         5.4319e-04, 1.6334e-04, 4.5069e-04, 1.4055e-02, 6.3419e-04, 3.3031e-02,\n",
      "         1.3551e-03, 1.0216e-04, 2.7624e-03, 7.7924e-04, 1.7903e-03, 4.3717e-04,\n",
      "         2.1390e-04, 6.6195e-04, 1.6589e-03, 7.2836e-03, 1.3044e-04, 3.3176e-04,\n",
      "         4.1294e-04, 3.8183e-03, 1.8784e-04, 2.2245e-03, 1.0911e-04, 1.3449e-04,\n",
      "         6.0990e-04, 2.1711e-04, 4.8498e-04, 9.3988e-05, 2.9922e-03, 1.1270e-04,\n",
      "         2.8642e-04, 2.7978e-04, 2.9463e-03, 3.8413e-03, 2.1582e-04, 9.7263e-04,\n",
      "         2.4733e-04, 1.0990e-04, 1.0618e-03, 1.0543e-04, 2.0182e-03, 1.7023e-04,\n",
      "         3.3968e-03, 3.4204e-04, 5.1352e-04, 2.9447e-04, 6.2496e-05, 1.2483e-04,\n",
      "         2.9744e-04, 4.1403e-04, 1.4595e-04, 1.6609e-04, 2.1250e-03, 1.6204e-04,\n",
      "         3.0578e-04, 5.6820e-04, 2.1681e-04, 3.1147e-04, 2.2899e-03, 4.2373e-03,\n",
      "         2.9807e-04, 2.6510e-04, 7.3791e-05, 8.2560e-05, 2.8295e-04, 2.5970e-03,\n",
      "         1.1428e-04, 9.7934e-05, 3.5565e-03, 2.5877e-04, 1.2696e-03, 1.4290e-04,\n",
      "         5.6618e-03, 1.5609e-04, 3.5549e-03, 1.8087e-03, 3.8238e-03, 1.0876e-02,\n",
      "         1.2199e-03, 1.9764e-04, 1.3570e-03, 3.6980e-04, 5.0356e-04, 8.9287e-03,\n",
      "         2.2475e-04, 2.4935e-04, 1.0981e-03, 3.0170e-04, 2.6413e-04, 4.4695e-04,\n",
      "         1.2187e-03, 9.8062e-04, 8.7889e-04, 1.8297e-03, 7.4873e-03, 1.7720e-04,\n",
      "         4.0209e-04, 8.7607e-05, 4.9188e-04, 2.7274e-03, 1.0781e-03, 8.1036e-05,\n",
      "         1.8310e-03, 8.4195e-04, 6.8700e-04, 8.2720e-04, 7.8465e-04, 4.3940e-03,\n",
      "         8.2270e-04, 3.5265e-04, 1.5848e-04, 2.2749e-04, 3.0721e-04, 3.0095e-04,\n",
      "         2.6518e-04, 1.5566e-04, 1.1676e-04, 1.1934e-03, 1.5246e-04, 6.8646e-04,\n",
      "         2.9613e-03, 1.0051e-02, 5.2783e-04, 3.6031e-04, 3.3777e-02, 1.1629e-04,\n",
      "         4.5091e-05, 5.0612e-04, 3.6486e-03, 3.9205e-04, 4.3716e-04, 2.9115e-03,\n",
      "         1.5039e-03, 3.0734e-04, 2.9559e-03, 1.3820e-03, 3.3793e-04, 8.8336e-04,\n",
      "         1.4710e-04, 2.1352e-03, 1.3657e-04, 8.1542e-04, 3.8085e-04, 3.7619e-04,\n",
      "         1.0723e-04, 1.5684e-04, 4.5717e-04, 5.9641e-02, 1.1574e-03, 7.8022e-04,\n",
      "         1.1535e-04, 5.1466e-04, 2.1573e-03, 1.3188e-04, 2.2334e-04, 6.6494e-03,\n",
      "         1.0424e-02, 1.1559e-02, 1.4101e-04, 3.9059e-03, 1.1624e-03, 8.6919e-05,\n",
      "         9.8605e-04, 1.5249e-03, 1.9077e-03, 6.4279e-03, 4.3572e-04, 1.2978e-04,\n",
      "         1.1945e-04, 8.7399e-05, 1.0315e-03, 1.3668e-03, 1.4714e-03, 8.6490e-04,\n",
      "         2.7076e-04, 8.6649e-04, 1.5161e-04, 9.4044e-04, 3.4900e-04, 2.7401e-04,\n",
      "         1.6091e-04, 1.9479e-04, 6.4210e-04, 4.7936e-04, 1.9181e-04, 2.4663e-04,\n",
      "         1.9210e-04, 8.1985e-04, 1.5692e-04, 1.6518e-03, 7.0178e-03]])\n",
      "\n",
      "Sentence: The Israeli cybersecurity company linked the attack to a group known as APT35, which is also tracked using the codenames Charming Kitten, Phosphorus, and TA453, citing overlaps with toolsets previously identified as infrastructure used by the threat actor.\n",
      "Predicted Label: T1027\n",
      "Prediction Probabilities: tensor([[1.4372e-04, 4.4356e-04, 7.3851e-04, 4.4010e-03, 3.1310e-04, 5.9425e-03,\n",
      "         1.0754e-03, 1.6183e-03, 4.3202e-04, 1.4363e-04, 1.3854e-03, 2.3446e-02,\n",
      "         1.0234e-03, 5.2122e-05, 3.8263e-05, 8.6197e-04, 1.0123e-04, 1.5843e-04,\n",
      "         1.0343e-04, 1.0925e-04, 4.2921e-05, 2.0363e-03, 2.1428e-04, 2.4270e-04,\n",
      "         4.0942e-04, 7.0242e-04, 3.3469e-03, 3.0244e-04, 5.9168e-05, 2.3701e-04,\n",
      "         1.8647e-04, 1.0368e-02, 1.4636e-04, 1.9171e-04, 6.5574e-05, 1.6282e-04,\n",
      "         6.8010e-05, 1.0075e-03, 1.5904e-03, 6.0419e-04, 6.6956e-05, 7.2908e-04,\n",
      "         9.1570e-04, 2.7542e-04, 2.2221e-03, 3.8537e-03, 1.4588e-04, 3.7080e-03,\n",
      "         2.6218e-04, 1.2584e-04, 9.0475e-05, 1.5785e-04, 4.5988e-04, 4.0829e-03,\n",
      "         4.4307e-03, 1.2401e-04, 5.2072e-04, 2.1513e-04, 1.1258e-04, 4.1963e-05,\n",
      "         2.3752e-04, 8.2658e-04, 6.1836e-05, 2.6524e-02, 6.4928e-05, 6.0273e-05,\n",
      "         1.8919e-04, 6.7240e-05, 3.5232e-04, 1.6963e-03, 1.8942e-05, 4.1631e-04,\n",
      "         1.3319e-03, 3.1102e-04, 2.5556e-04, 8.4718e-04, 2.4544e-04, 1.9730e-03,\n",
      "         3.9869e-02, 1.3394e-04, 1.6560e-04, 1.5277e-04, 4.5124e-04, 4.4345e-05,\n",
      "         1.8834e-03, 8.2455e-04, 1.4633e-03, 2.0675e-04, 9.7677e-05, 4.5688e-04,\n",
      "         1.5388e-03, 7.2031e-05, 1.2995e-03, 4.4862e-04, 2.4519e-02, 5.7008e-05,\n",
      "         9.0623e-04, 1.9767e-04, 2.5299e-04, 5.9367e-05, 5.1950e-05, 6.2466e-04,\n",
      "         5.7639e-03, 6.5838e-04, 4.8943e-04, 1.7028e-02, 3.3025e-03, 3.3380e-03,\n",
      "         1.8488e-04, 1.0630e-04, 1.9449e-04, 2.8507e-02, 2.4690e-04, 5.5499e-05,\n",
      "         3.5696e-04, 1.5838e-04, 5.3515e-05, 1.0425e-03, 2.1127e-04, 1.8969e-04,\n",
      "         8.8053e-04, 2.9030e-03, 9.9229e-04, 9.4614e-05, 3.3695e-04, 1.1373e-04,\n",
      "         1.1079e-04, 6.9159e-04, 1.6436e-03, 1.1989e-02, 4.4805e-05, 5.0430e-05,\n",
      "         1.6691e-04, 6.8210e-05, 1.0819e-04, 1.0379e-04, 1.5974e-03, 5.3721e-04,\n",
      "         1.3605e-04, 6.8942e-05, 9.7985e-04, 7.8520e-04, 2.4079e-04, 6.6366e-03,\n",
      "         1.2645e-04, 1.8888e-04, 2.9050e-03, 2.0520e-04, 3.5823e-04, 1.8440e-04,\n",
      "         7.0508e-03, 1.5234e-03, 1.5342e-03, 1.2137e-04, 1.1915e-04, 8.4131e-05,\n",
      "         3.4994e-04, 3.0415e-04, 1.5990e-02, 1.2528e-03, 2.0273e-04, 9.7160e-05,\n",
      "         5.2395e-04, 2.1037e-04, 4.3171e-05, 2.9354e-04, 1.2723e-04, 1.0179e-04,\n",
      "         1.5279e-03, 1.1348e-04, 1.1332e-04, 2.1159e-04, 1.2564e-03, 3.7924e-04,\n",
      "         1.0237e-02, 1.2431e-04, 1.0257e-04, 2.6144e-03, 4.6501e-04, 9.4472e-05,\n",
      "         5.7566e-04, 5.3407e-04, 4.5260e-04, 1.1280e-04, 3.6075e-05, 6.4677e-05,\n",
      "         3.9329e-04, 1.9091e-04, 4.3605e-04, 1.6534e-04, 3.4707e-03, 9.3396e-04,\n",
      "         2.9781e-04, 4.6274e-03, 6.6978e-05, 3.8792e-05, 9.3484e-04, 8.1355e-05,\n",
      "         1.0633e-04, 1.5441e-04, 6.9000e-03, 1.1520e-04, 6.8494e-04, 1.6208e-03,\n",
      "         7.2513e-03, 4.1783e-04, 2.2329e-04, 1.1609e-04, 4.0501e-04, 5.4367e-05,\n",
      "         3.0475e-04, 7.9415e-05, 2.6851e-04, 3.8036e-04, 7.8165e-04, 4.1926e-05,\n",
      "         1.7095e-02, 4.4738e-05, 1.2655e-03, 1.8400e-03, 1.8413e-03, 8.2806e-04,\n",
      "         8.8215e-05, 1.5374e-04, 2.0596e-03, 7.9057e-02, 1.7285e-04, 2.0384e-03,\n",
      "         5.4655e-03, 1.5894e-03, 2.1628e-04, 2.4013e-04, 3.2103e-04, 1.8531e-03,\n",
      "         5.1392e-04, 6.8262e-04, 5.4866e-05, 4.8771e-04, 3.8586e-04, 1.2400e-04,\n",
      "         2.0240e-04, 1.3430e-04, 4.6953e-04, 2.6485e-02, 1.6281e-04, 3.5994e-04,\n",
      "         3.3304e-04, 3.9018e-03, 8.3170e-04, 3.3806e-04, 2.2481e-04, 2.2257e-03,\n",
      "         5.0021e-05, 3.4446e-05, 2.9126e-03, 5.8267e-03, 1.5773e-04, 3.8183e-04,\n",
      "         2.7929e-04, 3.8830e-04, 2.8300e-03, 2.1772e-04, 4.6059e-03, 7.4271e-04,\n",
      "         4.2188e-04, 1.1452e-04, 7.3495e-04, 1.0500e-04, 5.5518e-03, 8.1034e-05,\n",
      "         1.6784e-03, 5.1851e-04, 1.2122e-03, 1.3598e-04, 3.9582e-04, 4.2964e-04,\n",
      "         1.6582e-04, 3.5675e-04, 4.6172e-03, 1.2642e-04, 1.0194e-04, 4.0428e-04,\n",
      "         5.8899e-04, 1.0419e-03, 4.8628e-04, 7.4156e-05, 8.0025e-04, 1.0871e-02,\n",
      "         2.5698e-04, 1.5634e-04, 3.1031e-04, 4.9112e-04, 3.6316e-04, 1.3647e-03,\n",
      "         7.8177e-05, 5.4675e-05, 1.7834e-04, 2.8833e-04, 1.9165e-04, 1.0906e-04,\n",
      "         5.9262e-05, 2.1374e-04, 3.8919e-04, 3.1323e-04, 8.7542e-04, 5.1355e-04,\n",
      "         3.6465e-04, 2.4860e-04, 4.7529e-03, 3.4508e-04, 1.1839e-03, 1.3104e-04,\n",
      "         2.2208e-04, 1.6120e-04, 1.3000e-04, 7.0476e-04, 7.9528e-04, 2.9419e-04,\n",
      "         4.4415e-04, 1.3534e-04, 3.0176e-04, 8.3785e-03, 3.2111e-04, 2.3895e-02,\n",
      "         8.5136e-04, 4.7226e-05, 1.0852e-02, 8.0955e-04, 1.1964e-03, 3.0851e-04,\n",
      "         1.1379e-04, 8.2954e-04, 3.6262e-03, 2.5696e-03, 5.7762e-05, 2.4225e-04,\n",
      "         3.9372e-04, 4.3602e-03, 1.1686e-04, 7.3480e-04, 9.6523e-05, 8.0348e-05,\n",
      "         3.1289e-04, 1.3794e-04, 2.4953e-04, 5.7857e-05, 1.4170e-02, 6.4852e-05,\n",
      "         2.0820e-04, 1.1873e-04, 5.6348e-03, 7.9978e-03, 1.3180e-04, 4.5359e-04,\n",
      "         1.9761e-04, 7.7045e-05, 1.3234e-03, 4.8914e-05, 9.4492e-04, 1.0022e-04,\n",
      "         3.0691e-03, 2.4875e-04, 3.4160e-04, 2.8653e-04, 4.8601e-05, 6.3152e-05,\n",
      "         1.6817e-04, 3.4578e-04, 7.2657e-05, 9.0723e-05, 9.1694e-04, 7.6221e-05,\n",
      "         1.7850e-04, 4.1349e-04, 8.7156e-05, 1.3346e-04, 1.9386e-03, 1.9096e-03,\n",
      "         1.3435e-04, 2.1053e-04, 5.3665e-05, 7.6190e-05, 2.4505e-04, 2.6266e-03,\n",
      "         6.6603e-05, 7.8372e-05, 1.2281e-03, 1.2422e-04, 5.7975e-04, 4.4637e-05,\n",
      "         6.7151e-03, 1.3016e-04, 2.3139e-03, 1.4260e-03, 1.6715e-03, 6.7939e-03,\n",
      "         1.0922e-03, 1.1546e-04, 1.2813e-03, 2.9647e-04, 2.5038e-04, 1.2325e-02,\n",
      "         1.4955e-04, 1.5793e-04, 7.3965e-04, 1.5089e-04, 1.6505e-04, 2.8754e-04,\n",
      "         7.9838e-04, 8.7572e-04, 4.6137e-04, 2.2313e-03, 3.8371e-03, 1.3422e-04,\n",
      "         2.8975e-04, 6.2062e-05, 5.7340e-04, 2.3637e-03, 1.5376e-03, 7.6367e-05,\n",
      "         2.8084e-03, 4.1468e-04, 6.4421e-04, 8.8817e-04, 5.1009e-04, 2.5329e-03,\n",
      "         9.7767e-04, 3.1005e-04, 9.3544e-05, 1.2258e-04, 1.8362e-04, 1.8293e-04,\n",
      "         1.2809e-04, 1.1877e-04, 7.9645e-05, 9.4800e-04, 1.3669e-04, 5.3892e-04,\n",
      "         3.8681e-03, 1.2310e-02, 6.7895e-04, 1.5280e-04, 1.0699e-02, 9.8614e-05,\n",
      "         2.8306e-05, 3.5206e-04, 1.8513e-03, 2.0928e-04, 4.3048e-04, 2.6720e-03,\n",
      "         1.0548e-03, 1.5153e-04, 1.5742e-03, 8.0819e-04, 3.0924e-04, 1.1750e-03,\n",
      "         8.2297e-05, 1.6284e-03, 7.5736e-05, 1.3400e-03, 3.3302e-04, 2.6202e-04,\n",
      "         6.9798e-05, 8.8834e-05, 2.9298e-04, 1.2944e-01, 2.0457e-03, 9.9364e-04,\n",
      "         6.4421e-05, 3.5458e-04, 1.0243e-03, 1.0357e-04, 1.9147e-04, 7.2300e-03,\n",
      "         1.0856e-02, 4.8214e-03, 1.1064e-04, 1.2983e-02, 6.7321e-04, 5.4926e-05,\n",
      "         3.9181e-04, 1.6858e-03, 2.1677e-03, 2.8408e-03, 2.2840e-04, 1.0000e-04,\n",
      "         8.6401e-05, 6.1079e-05, 4.9870e-04, 1.0195e-03, 1.3739e-03, 8.0386e-04,\n",
      "         1.6327e-04, 3.3017e-04, 1.0185e-04, 8.4643e-04, 3.1540e-04, 1.9598e-04,\n",
      "         1.1605e-04, 1.2523e-04, 7.0379e-04, 5.0289e-04, 1.2249e-04, 1.3636e-04,\n",
      "         3.0487e-04, 1.3062e-03, 9.9767e-05, 2.7663e-03, 2.2815e-03]])\n",
      "\n",
      "Sentence: Cybersecurity Log4Shell aka CVE-2021-44228 (CVSS score: 10.0) concerns a critical security vulnerability in the popular Log4j logging library that, if successfully exploited, could lead to remote execution of arbitrary code on compromised systems.\n",
      "Predicted Label: T1203\n",
      "Prediction Probabilities: tensor([[4.5600e-04, 6.7992e-04, 1.9801e-03, 7.5741e-04, 7.6307e-04, 4.5670e-03,\n",
      "         9.0754e-04, 1.4284e-03, 1.0118e-03, 5.4780e-04, 3.3420e-03, 1.4380e-03,\n",
      "         7.3174e-04, 3.3525e-04, 4.5662e-04, 7.7684e-04, 4.4364e-04, 7.4912e-04,\n",
      "         3.1451e-04, 4.4601e-04, 5.9458e-04, 2.0890e-03, 5.4464e-04, 1.2018e-03,\n",
      "         7.5193e-04, 9.3502e-04, 1.6910e-03, 7.5252e-04, 1.8902e-04, 5.3372e-04,\n",
      "         1.1072e-03, 5.6905e-03, 3.6558e-04, 4.3931e-04, 3.0304e-04, 5.9993e-04,\n",
      "         4.7184e-04, 9.4343e-04, 5.3600e-04, 8.4814e-04, 2.5613e-04, 7.1118e-04,\n",
      "         1.2960e-03, 4.2948e-04, 1.6427e-03, 1.6608e-03, 5.6388e-04, 2.7495e-03,\n",
      "         3.3037e-04, 6.0358e-04, 3.9711e-04, 6.2387e-04, 6.9441e-04, 1.0523e-03,\n",
      "         1.4164e-03, 4.2093e-04, 5.2207e-04, 8.6008e-04, 3.7926e-04, 3.1307e-04,\n",
      "         9.7601e-04, 8.4104e-04, 4.4812e-04, 1.7321e-03, 5.3820e-04, 3.0384e-04,\n",
      "         8.8385e-04, 2.7998e-04, 4.6301e-04, 6.1537e-04, 2.5587e-04, 3.6930e-04,\n",
      "         1.0718e-03, 8.0262e-04, 6.6943e-04, 8.8383e-04, 8.1193e-04, 8.8138e-04,\n",
      "         2.3366e-03, 4.7925e-04, 2.9494e-04, 5.8683e-04, 5.6516e-04, 4.8769e-04,\n",
      "         1.8217e-03, 1.0151e-03, 4.6941e-04, 4.3487e-04, 4.3137e-04, 7.5743e-04,\n",
      "         6.9401e-04, 3.2127e-04, 1.1274e-03, 8.0409e-04, 1.7024e-03, 3.6751e-04,\n",
      "         8.0361e-04, 3.6071e-04, 3.9261e-04, 5.1495e-04, 4.0437e-04, 6.9608e-04,\n",
      "         1.0560e-03, 5.6028e-04, 6.6449e-04, 3.1736e-03, 7.8887e-04, 3.6597e-03,\n",
      "         3.6283e-04, 9.2597e-04, 5.5749e-04, 9.9024e-04, 7.5356e-04, 3.6402e-04,\n",
      "         1.0474e-03, 5.8179e-04, 2.8584e-04, 6.1206e-04, 5.5277e-04, 5.5331e-04,\n",
      "         5.5247e-04, 1.0779e-03, 7.2950e-04, 3.9457e-04, 6.6808e-04, 4.9058e-04,\n",
      "         5.9825e-04, 5.5093e-04, 2.3118e-03, 3.5102e-03, 4.5433e-04, 3.2073e-04,\n",
      "         7.6251e-04, 2.5993e-04, 7.3600e-04, 3.5293e-04, 1.0326e-03, 8.0728e-04,\n",
      "         6.0012e-04, 2.1906e-04, 1.2827e-03, 6.0620e-04, 5.2124e-04, 1.3979e-03,\n",
      "         4.6134e-04, 5.0892e-04, 1.3255e-03, 6.3398e-04, 6.5821e-04, 5.4722e-04,\n",
      "         1.8506e-03, 4.7928e-04, 1.5505e-03, 4.6551e-04, 7.0712e-04, 5.2255e-04,\n",
      "         9.3181e-04, 6.1150e-04, 5.2213e-03, 6.4768e-04, 4.6056e-04, 5.1547e-04,\n",
      "         7.2944e-04, 5.5494e-04, 4.0592e-04, 9.5927e-04, 4.4812e-04, 2.6300e-04,\n",
      "         1.0923e-03, 2.3597e-04, 4.5610e-04, 5.7870e-04, 6.8706e-04, 5.5910e-04,\n",
      "         1.5139e-03, 6.3344e-04, 4.7696e-04, 1.2001e-03, 9.5239e-04, 3.8007e-04,\n",
      "         1.2267e-03, 9.5414e-04, 1.0388e-03, 7.1808e-04, 2.8289e-04, 3.3375e-04,\n",
      "         4.1206e-04, 4.9594e-04, 6.3492e-04, 5.5647e-04, 1.3985e-03, 7.4713e-04,\n",
      "         7.2655e-04, 5.4290e-04, 5.4952e-04, 4.2248e-04, 1.0306e-03, 2.5001e-04,\n",
      "         3.9724e-04, 4.4966e-04, 2.3418e-03, 2.4533e-04, 5.8438e-04, 1.0243e-03,\n",
      "         8.6903e-04, 1.0551e-03, 6.8380e-04, 7.5926e-04, 8.4898e-04, 3.2159e-04,\n",
      "         6.8557e-04, 3.8824e-04, 1.1689e-03, 6.2848e-04, 7.0974e-04, 2.0794e-04,\n",
      "         1.9032e-03, 2.3856e-04, 2.0199e-03, 1.3488e-03, 1.8249e-03, 7.3358e-04,\n",
      "         7.9210e-04, 3.7589e-04, 1.5412e-03, 2.5295e-03, 4.8593e-04, 1.2170e-03,\n",
      "         9.4463e-04, 1.3927e-03, 8.8644e-04, 8.8485e-04, 6.6603e-04, 1.2545e-03,\n",
      "         6.3288e-04, 5.3930e-04, 3.2248e-04, 6.3975e-04, 4.8404e-04, 5.2212e-04,\n",
      "         5.5345e-04, 5.5271e-04, 4.6598e-04, 1.4850e-03, 5.3668e-04, 5.4053e-04,\n",
      "         8.1667e-04, 1.6321e-03, 7.8883e-04, 7.1390e-04, 5.9402e-04, 3.9698e-04,\n",
      "         3.2260e-04, 2.9155e-04, 8.9632e-04, 1.5889e-03, 7.2668e-04, 3.9246e-04,\n",
      "         8.6976e-04, 7.4770e-04, 8.7968e-04, 1.0854e-03, 1.9645e-03, 1.6043e-03,\n",
      "         9.1299e-04, 3.3195e-04, 5.9556e-04, 8.5835e-04, 2.1272e-03, 2.7485e-04,\n",
      "         1.1719e-03, 7.7247e-04, 7.4133e-04, 6.5370e-04, 8.0324e-04, 9.1261e-04,\n",
      "         6.4458e-04, 5.8639e-04, 1.3489e-03, 2.6579e-04, 6.4757e-04, 8.2353e-04,\n",
      "         3.2386e-03, 1.0525e-03, 8.7326e-04, 2.8789e-04, 1.5598e-02, 1.4424e-03,\n",
      "         5.7038e-04, 8.9106e-04, 3.6408e-04, 8.1051e-04, 8.1376e-04, 7.3423e-04,\n",
      "         3.1775e-04, 3.3046e-04, 8.0401e-04, 7.4142e-04, 3.1188e-04, 5.2250e-04,\n",
      "         2.7641e-04, 3.4527e-04, 7.3406e-04, 7.2426e-04, 1.0542e-03, 3.6488e-04,\n",
      "         6.3446e-04, 2.9984e-04, 1.2424e-03, 7.9953e-04, 1.2141e-03, 4.8764e-04,\n",
      "         3.2112e-04, 4.6175e-04, 4.0555e-04, 7.9027e-04, 9.2887e-04, 3.9309e-04,\n",
      "         8.3460e-04, 4.7710e-04, 6.1925e-04, 2.0216e-03, 6.7131e-04, 1.1425e-03,\n",
      "         9.7507e-04, 5.1639e-04, 1.4564e-03, 1.2339e-03, 7.3122e-04, 5.5902e-04,\n",
      "         7.6684e-04, 5.4644e-04, 2.3512e-03, 1.1297e-03, 4.7503e-04, 7.7479e-04,\n",
      "         9.6022e-04, 5.3272e-04, 4.5413e-04, 1.8538e-03, 4.6608e-04, 6.4484e-04,\n",
      "         4.3774e-04, 9.4714e-04, 3.1436e-04, 2.2904e-04, 8.5761e-04, 6.0714e-04,\n",
      "         4.8428e-04, 3.9219e-04, 3.5581e-03, 9.3728e-04, 4.6888e-04, 7.8662e-04,\n",
      "         5.0204e-04, 5.1798e-04, 9.1235e-04, 2.0330e-04, 9.4147e-04, 4.5652e-04,\n",
      "         9.0049e-02, 8.0088e-04, 6.7739e-04, 4.7075e-04, 3.2731e-04, 3.8871e-04,\n",
      "         6.1745e-04, 5.5000e-04, 4.5019e-04, 4.2651e-04, 7.9692e-04, 5.2311e-04,\n",
      "         5.3580e-04, 5.5418e-04, 3.4234e-04, 6.5693e-04, 1.2630e-03, 2.0586e-03,\n",
      "         4.1264e-04, 4.0922e-04, 1.9689e-04, 3.5539e-04, 5.4953e-04, 1.4702e-03,\n",
      "         3.1893e-04, 2.7656e-04, 1.9630e-03, 3.2288e-04, 1.0439e-03, 5.1528e-04,\n",
      "         1.0471e-03, 7.8710e-04, 7.0769e-04, 1.2416e-03, 1.4021e-03, 4.3944e-04,\n",
      "         4.8238e-04, 5.3546e-04, 1.7514e-03, 6.9220e-04, 5.7619e-04, 2.5164e-03,\n",
      "         4.1352e-04, 9.8938e-04, 7.5005e-03, 6.3655e-04, 7.5132e-04, 5.8648e-04,\n",
      "         1.0506e-03, 1.6102e-03, 7.4815e-04, 1.2933e-03, 5.5782e-04, 5.5098e-04,\n",
      "         7.6804e-04, 3.0061e-04, 1.2757e-03, 1.4937e-03, 1.4384e-03, 2.5739e-04,\n",
      "         1.0273e-03, 8.2537e-04, 9.7386e-04, 5.2120e-04, 8.3741e-04, 1.1004e-03,\n",
      "         8.0003e-04, 9.9957e-04, 7.9757e-04, 6.3633e-04, 4.0848e-04, 3.4063e-04,\n",
      "         6.7190e-04, 1.9106e-04, 5.0885e-04, 9.4411e-04, 9.5160e-04, 4.4145e-04,\n",
      "         1.7848e-03, 1.1678e-03, 1.2021e-03, 5.3323e-04, 1.2029e-03, 3.8136e-04,\n",
      "         3.8174e-04, 5.7177e-04, 7.5526e-04, 3.6825e-04, 1.0348e-03, 1.5653e-03,\n",
      "         5.1265e-04, 3.8845e-04, 1.9705e-01, 1.6658e-03, 6.6503e-04, 7.4341e-04,\n",
      "         3.0907e-04, 1.3393e-03, 3.1429e-04, 1.9380e-03, 4.5022e-04, 7.8376e-04,\n",
      "         3.6827e-04, 4.8380e-04, 6.5638e-04, 2.5374e-03, 1.4433e-03, 5.8565e-04,\n",
      "         3.3102e-04, 4.7448e-04, 6.0832e-04, 5.1070e-04, 5.9894e-04, 5.4540e-03,\n",
      "         1.4121e-03, 1.4566e-03, 4.5239e-04, 2.6291e-01, 9.0288e-04, 3.0883e-04,\n",
      "         8.2391e-04, 1.3624e-03, 1.4115e-03, 3.8644e-03, 4.5568e-04, 4.0377e-04,\n",
      "         4.1156e-04, 3.0286e-04, 7.5990e-04, 9.5664e-04, 1.1556e-03, 1.0409e-03,\n",
      "         6.4110e-04, 1.2899e-03, 5.6614e-04, 3.7953e-04, 4.8285e-04, 4.9266e-04,\n",
      "         5.9543e-04, 3.3745e-04, 4.8402e-04, 6.4078e-04, 5.5419e-04, 4.7967e-04,\n",
      "         6.1747e-04, 1.9404e-03, 3.1912e-04, 1.5480e-03, 3.6654e-04]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./final_model1')  # Replace with your model path\n",
    "tokenizer = BertTokenizer.from_pretrained('ibm-research/CTI-BERT')  # Replace with your tokenizer if needed\n",
    "\n",
    "# # Assuming you have a list of labels (e.g., ['Label_0', 'Label_1', 'Label_2'])\n",
    "# labels = model.config.id2label # Update with your actual labels\n",
    "# print(labels)\n",
    "# Function to get prediction for a given sentence\n",
    "def predict(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradients for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    predicted_label = torch.argmax(probabilities, dim=-1).item()\n",
    "    \n",
    "    return predicted_label, probabilities\n",
    "\n",
    "# Function to break text into sentences using spaCy\n",
    "def sentence_tokenizer_spacy(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]  # Strip leading/trailing spaces\n",
    "    return sentences\n",
    "\n",
    "# Example news article\n",
    "input_text = \"\"\"\n",
    "An Iranian state-sponsored actor has been observed scanning and attempting to abuse the Log4Shell flaw in publicly-exposed Java applications to deploy a hitherto undocumented PowerShell-based modular backdoor dubbed \"CharmPower\" for follow-on post-exploitation. \"The actor's attack setup was obviously rushed, as they used the basic open-source tool for the exploitation and based their operations on previous infrastructure, which made the attack easier to detect and attribute,\" researchers from Check Point said in a report published this week. The Israeli cybersecurity company linked the attack to a group known as APT35, which is also tracked using the codenames Charming Kitten, Phosphorus, and TA453, citing overlaps with toolsets previously identified as infrastructure used by the threat actor. Cybersecurity Log4Shell aka CVE-2021-44228 (CVSS score: 10.0) concerns a critical security vulnerability in the popular Log4j logging library that, if successfully exploited, could lead to remote execution of arbitrary code on compromised systems.\n",
    "\"\"\"\n",
    "\n",
    "# Break the article into sentences using spaCy\n",
    "sentences = sentence_tokenizer_spacy(input_text)\n",
    "\n",
    "# Process each sentence through the model and get predictions\n",
    "for sentence in sentences:\n",
    "    predicted_label, probabilities = predict(sentence)\n",
    "    \n",
    "    # Map the predicted label index to the label name\n",
    "    predicted_label_name = labels[predicted_label]\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Predicted Label: {predicted_label_name}\")\n",
    "    print(f\"Prediction Probabilities: {probabilities}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42606028-554a-464a-aedc-504255b78f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: NDCG@5 = 0.0000\n",
      "Sample 1: NDCG@5 = 0.0000\n",
      "Sample 2: NDCG@5 = 0.0000\n",
      "Sample 3: NDCG@5 = 0.0000\n",
      "Sample 4: NDCG@5 = 0.0000\n",
      "Sample 5: NDCG@5 = 0.0000\n",
      "Sample 6: NDCG@5 = 0.0000\n",
      "Sample 7: NDCG@5 = 0.0000\n",
      "Sample 8: NDCG@5 = 0.0000\n",
      "Sample 9: NDCG@5 = 0.0000\n",
      "\n",
      "Mean NDCG@5 over dataset = 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('./final_model1')\n",
    "tokenizer = BertTokenizer.from_pretrained('ibm-research/CTI-BERT')\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Your label map\n",
    "labels = [...]  # List of label names, e.g., ['T1190', 'T1078', 'T1059', ...]\n",
    "label_map = {label: idx for idx, label in enumerate(labels)}\n",
    "label_map_rev = {idx: label for label, idx in label_map.items()}\n",
    "\n",
    "# Prediction function\n",
    "def predict_full_text(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    return probabilities.squeeze().cpu().numpy()  # shape (num_classes,)\n",
    "\n",
    "# NDCG computation\n",
    "def compute_ndcg_for_sample(probs, true_labels, label_map, k=5):\n",
    "    num_classes = probs.shape[0]\n",
    "    true_relevance = np.zeros((1, num_classes))\n",
    "    for label in true_labels:\n",
    "        idx = label_map.get(label)\n",
    "        if idx is not None:\n",
    "            true_relevance[0, idx] = 1  # mark relevant classes\n",
    "    probs = probs.reshape(1, -1)\n",
    "    return ndcg_score(true_relevance, probs, k=k)\n",
    "\n",
    "# Load your CSS-like dataset\n",
    "data = pd.read_csv('real_news_maps.csv')  # 'news' and 'maps' columns\n",
    "\n",
    "all_ndcg_scores = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    text = row['news']\n",
    "    true_labels = eval(row['maps'])  # Convert string to list\n",
    "    \n",
    "    probs = predict_full_text(text)\n",
    "\n",
    "    ndcg = compute_ndcg_for_sample(probs, true_labels, label_map, k=20)\n",
    "\n",
    "    all_ndcg_scores.append(ndcg)\n",
    "    print(f\"Sample {idx}: NDCG@5 = {ndcg:.4f}\")\n",
    "\n",
    "# After all samples\n",
    "mean_ndcg = np.mean(all_ndcg_scores)\n",
    "print(f\"\\nMean NDCG@5 over dataset = {mean_ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c5e04-d167-4b9b-aa1b-86fd596548ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a24f3d-1371-47d2-b75f-f099e896ac9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4df59-52f3-48b0-88b3-bbd538e3f244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
